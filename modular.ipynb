{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 2187 chunks.\n",
      "\n",
      "First 10 chunks:\n",
      "    chapter                                              chunk  \\\n",
      "0  Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
      "1  Preamble                             (Tom Sawyer’s Comrade)   \n",
      "2  Preamble                                      By Mark Twain   \n",
      "3  Preamble                                            NOTICE.   \n",
      "4  Preamble  Persons attempting to find a motive in this na...   \n",
      "5  Preamble  BY ORDER OF THE AUTHOR\\nPER G. G., CHIEF OF OR...   \n",
      "6  Preamble                                        EXPLANATORY   \n",
      "7  Preamble  In this book a number of dialects are used, to...   \n",
      "8  Preamble  I make this explanation for the reason that wi...   \n",
      "9  Preamble                                        THE AUTHOR.   \n",
      "\n",
      "   chunk_order_number  \n",
      "0                   1  \n",
      "1                   2  \n",
      "2                   3  \n",
      "3                   4  \n",
      "4                   5  \n",
      "5                   6  \n",
      "6                   7  \n",
      "7                   8  \n",
      "8                   9  \n",
      "9                  10  \n",
      "\n",
      "Last 10 chunks:\n",
      "            chapter                                              chunk  \\\n",
      "2177  CHAPTER XLII.  “No, he hain’t,” Tom says; “it’s all there yet...   \n",
      "2178  CHAPTER XLII.                          Jim says, kind of solemn:   \n",
      "2179  CHAPTER XLII.             “He ain’t a-comin’ back no mo’, Huck.”   \n",
      "2180  CHAPTER XLII.                                            I says:   \n",
      "2181  CHAPTER XLII.                                        “Why, Jim?”   \n",
      "2182  CHAPTER XLII.  “Nemmine why, Huck—but he ain’t comin’ back no...   \n",
      "2183  CHAPTER XLII.             But I kept at him; so at last he says:   \n",
      "2184  CHAPTER XLII.  “Doan’ you ’member de house dat was float’n do...   \n",
      "2185  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
      "2186  CHAPTER XLII.                 THE END. YOURS TRULY, _HUCK FINN_.   \n",
      "\n",
      "      chunk_order_number  \n",
      "2177                2178  \n",
      "2178                2179  \n",
      "2179                2180  \n",
      "2180                2181  \n",
      "2181                2182  \n",
      "2182                2183  \n",
      "2183                2184  \n",
      "2184                2185  \n",
      "2185                2186  \n",
      "2186                2187  \n",
      "\n",
      "All chunks from CHAPTER II.:\n",
      "        chapter                                              chunk  \\\n",
      "21  CHAPTER II.  We went tiptoeing along a path amongst the tre...   \n",
      "22  CHAPTER II.                                         “Who dah?”   \n",
      "23  CHAPTER II.  He listened some more; then he come tiptoeing ...   \n",
      "24  CHAPTER II.  “Say, who is you? Whar is you? Dog my cats ef ...   \n",
      "25  CHAPTER II.  So he set down on the ground betwixt me and To...   \n",
      "26  CHAPTER II.  Tom he made a sign to me—kind of a little nois...   \n",
      "27  CHAPTER II.  As soon as Tom was back we cut along the path,...   \n",
      "28  CHAPTER II.  Well, when Tom and me got to the edge of the h...   \n",
      "29  CHAPTER II.  We went to a clump of bushes, and Tom made eve...   \n",
      "30  CHAPTER II.  “Now, we’ll start this band of robbers and cal...   \n",
      "31  CHAPTER II.  Everybody was willing. So Tom got out a sheet ...   \n",
      "32  CHAPTER II.  Everybody said it was a real beautiful oath, a...   \n",
      "33  CHAPTER II.  Some thought it would be good to kill the _fam...   \n",
      "34  CHAPTER II.  “Here’s Huck Finn, he hain’t got no family; wh...   \n",
      "35  CHAPTER II.   “Well, hain’t he got a father?” says Tom Sawyer.   \n",
      "36  CHAPTER II.  “Yes, he’s got a father, but you can’t never f...   \n",
      "37  CHAPTER II.  They talked it over, and they was going to rul...   \n",
      "38  CHAPTER II.  “Oh, she’ll do. That’s all right. Huck can com...   \n",
      "39  CHAPTER II.  Then they all stuck a pin in their fingers to ...   \n",
      "40  CHAPTER II.  “Now,” says Ben Rogers, “what’s the line of bu...   \n",
      "41  CHAPTER II.       “Nothing only robbery and murder,” Tom said.   \n",
      "42  CHAPTER II.  “But who are we going to rob?—houses, or cattl...   \n",
      "43  CHAPTER II.  “Stuff! stealing cattle and such things ain’t ...   \n",
      "44  CHAPTER II.                  “Must we always kill the people?”   \n",
      "45  CHAPTER II.  “Oh, certainly. It’s best. Some authorities th...   \n",
      "46  CHAPTER II.                           “Ransomed? What’s that?”   \n",
      "47  CHAPTER II.  “I don’t know. But that’s what they do. I’ve s...   \n",
      "48  CHAPTER II.  “But how can we do it if we don’t know what it...   \n",
      "49  CHAPTER II.  “Why, blame it all, we’ve _got_ to do it. Don’...   \n",
      "50  CHAPTER II.  “Oh, that’s all very fine to _say_, Tom Sawyer...   \n",
      "51  CHAPTER II.  “Well, I don’t know. But per’aps if we keep th...   \n",
      "52  CHAPTER II.  “Now, that’s something _like_. That’ll answer....   \n",
      "53  CHAPTER II.  “How you talk, Ben Rogers. How can they get lo...   \n",
      "54  CHAPTER II.  “A guard! Well, that _is_ good. So somebody’s ...   \n",
      "55  CHAPTER II.  “Because it ain’t in the books so—that’s why. ...   \n",
      "56  CHAPTER II.  “All right. I don’t mind; but I say it’s a foo...   \n",
      "57  CHAPTER II.  “Well, Ben Rogers, if I was as ignorant as you...   \n",
      "58  CHAPTER II.  “Well, if that’s the way I’m agreed, but I don...   \n",
      "59  CHAPTER II.  Little Tommy Barnes was asleep now, and when t...   \n",
      "60  CHAPTER II.  So they all made fun of him, and called him cr...   \n",
      "61  CHAPTER II.  Ben Rogers said he couldn’t get out much, only...   \n",
      "62  CHAPTER II.  I clumb up the shed and crept into my window j...   \n",
      "\n",
      "    chunk_order_number  \n",
      "21                  22  \n",
      "22                  23  \n",
      "23                  24  \n",
      "24                  25  \n",
      "25                  26  \n",
      "26                  27  \n",
      "27                  28  \n",
      "28                  29  \n",
      "29                  30  \n",
      "30                  31  \n",
      "31                  32  \n",
      "32                  33  \n",
      "33                  34  \n",
      "34                  35  \n",
      "35                  36  \n",
      "36                  37  \n",
      "37                  38  \n",
      "38                  39  \n",
      "39                  40  \n",
      "40                  41  \n",
      "41                  42  \n",
      "42                  43  \n",
      "43                  44  \n",
      "44                  45  \n",
      "45                  46  \n",
      "46                  47  \n",
      "47                  48  \n",
      "48                  49  \n",
      "49                  50  \n",
      "50                  51  \n",
      "51                  52  \n",
      "52                  53  \n",
      "53                  54  \n",
      "54                  55  \n",
      "55                  56  \n",
      "56                  57  \n",
      "57                  58  \n",
      "58                  59  \n",
      "59                  60  \n",
      "60                  61  \n",
      "61                  62  \n",
      "62                  63  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def novel_to_dataframe(novel_text: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the full plain text of a novel, identifies chapters,\n",
    "    chunks the text by paragraphs within each chapter, and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        novel_text: A string containing the full plain text of the novel.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with columns 'chapter', 'chunk', and 'chunk_order_number'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Optional: Attempt to remove Project Gutenberg headers/footers\n",
    "    # Define common markers\n",
    "    start_marker_pattern = r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK [^*]+\\*\\*\\*\"\n",
    "    end_marker_pattern = r\"\\*\\*\\* END OF THE PROJECT GUTENBERG EBOOK [^*]+\\*\\*\\*\"\n",
    "\n",
    "    # Remove text before the start marker\n",
    "    start_match = re.search(start_marker_pattern, novel_text)\n",
    "    if start_match:\n",
    "        novel_text = novel_text[start_match.end():]\n",
    "\n",
    "    # Remove text after the end marker\n",
    "    end_match = re.search(end_marker_pattern, novel_text)\n",
    "    if end_match:\n",
    "        novel_text = novel_text[:end_match.start()]\n",
    "\n",
    "    novel_text = novel_text.strip()\n",
    "\n",
    "    chapters_data = []\n",
    "    chunk_order_counter = 0  # Initialize chunk order counter\n",
    "\n",
    "    # Regex to find chapter titles like \"CHAPTER I.\", \"CHAPTER II.\", etc.\n",
    "    # It assumes chapter titles are on their own line.\n",
    "    chapter_pattern = re.compile(r\"^(CHAPTER [IVXLCDM]+\\.)\", re.MULTILINE)\n",
    "\n",
    "    matches = list(chapter_pattern.finditer(novel_text))\n",
    "\n",
    "    if not matches:\n",
    "        # If no chapters are found according to the pattern,\n",
    "        # treat the whole text as chunks under an 'Unknown' chapter.\n",
    "        paragraphs = re.split(r'\\n\\s*\\n+', novel_text)\n",
    "        for para_content in paragraphs:\n",
    "            para_cleaned = para_content.strip()\n",
    "            if para_cleaned:\n",
    "                chunk_order_counter += 1\n",
    "                chapters_data.append({\n",
    "                    'chapter': 'Unknown',\n",
    "                    'chunk': para_cleaned,\n",
    "                    'chunk_order_number': chunk_order_counter\n",
    "                })\n",
    "        if chapters_data:\n",
    "            return pd.DataFrame(chapters_data)\n",
    "        else: # If text was empty or only whitespace\n",
    "            return pd.DataFrame(columns=['chapter', 'chunk', 'chunk_order_number'])\n",
    "\n",
    "    # Process text before the first chapter, if any\n",
    "    first_chapter_start_index = matches[0].start()\n",
    "    text_before_first_chapter = novel_text[:first_chapter_start_index].strip()\n",
    "    if text_before_first_chapter:\n",
    "        paragraphs_before = re.split(r'\\n\\s*\\n+', text_before_first_chapter)\n",
    "        for para_content in paragraphs_before:\n",
    "            para_cleaned = para_content.strip()\n",
    "            if para_cleaned:\n",
    "                chunk_order_counter += 1\n",
    "                chapters_data.append({\n",
    "                    'chapter': 'Preamble', # Or 'Introduction', 'Unknown', etc.\n",
    "                    'chunk': para_cleaned,\n",
    "                    'chunk_order_number': chunk_order_counter\n",
    "                })\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        chapter_title = match.group(1)  # e.g., \"CHAPTER I.\"\n",
    "\n",
    "        # Content starts after the current chapter title's line\n",
    "        content_start_index = match.end()\n",
    "\n",
    "        # Content ends at the start of the next chapter title, or at the end of the text\n",
    "        if i + 1 < len(matches):\n",
    "            content_end_index = matches[i+1].start()\n",
    "        else:\n",
    "            content_end_index = len(novel_text)\n",
    "\n",
    "        chapter_content = novel_text[content_start_index:content_end_index].strip()\n",
    "\n",
    "        # Split chapter content into paragraphs (chunks)\n",
    "        paragraphs = re.split(r'\\n\\s*\\n+', chapter_content)\n",
    "\n",
    "        for para_content in paragraphs:\n",
    "            para_cleaned = para_content.strip()\n",
    "            if para_cleaned:  # Add only non-empty paragraphs\n",
    "                chunk_order_counter += 1\n",
    "                chapters_data.append({\n",
    "                    'chapter': chapter_title,\n",
    "                    'chunk': para_cleaned,\n",
    "                    'chunk_order_number': chunk_order_counter\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(chapters_data)\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use the example text you provided (truncated for brevity here)\n",
    "    file_path = \"/Users/davidspencer/Downloads/memgraph_import/Adventures of Huckleberry Finn.txt\" # Replace with the actual path to your .txt file\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        huckleberry_finn_text = file.read()\n",
    "\n",
    "\n",
    "    # You would continue with CHAPTER III, IV, V, VI, VII... and the rest of the novel.\n",
    "    # For this example, we're using the truncated text above.\n",
    "\n",
    "    # Process the text\n",
    "    df_novel = novel_to_dataframe(huckleberry_finn_text)\n",
    "\n",
    "    # Display the DataFrame (or parts of it)\n",
    "    print(f\"Successfully processed {len(df_novel)} chunks.\")\n",
    "    print(\"\\nFirst 10 chunks:\")\n",
    "    print(df_novel.head(10))\n",
    "\n",
    "    print(\"\\nLast 10 chunks:\")\n",
    "    print(df_novel.tail(10))\n",
    "\n",
    "    # Example: Show all chunks from a specific chapter\n",
    "    if not df_novel.empty and 'CHAPTER II.' in df_novel['chapter'].unique():\n",
    "        print(\"\\nAll chunks from CHAPTER II.:\")\n",
    "        print(df_novel[df_novel['chapter'] == 'CHAPTER II.'])\n",
    "    elif not df_novel.empty:\n",
    "        print(f\"\\nChunks found, but 'CHAPTER II.' not present in the sample used. Available chapters: {df_novel['chapter'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_order_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>ADVENTURES\\nOF\\nHUCKLEBERRY FINN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>(Tom Sawyer’s Comrade)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>By Mark Twain</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>NOTICE.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>Persons attempting to find a motive in this na...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Nemmine why, Huck—but he ain’t comin’ back no...</td>\n",
       "      <td>2183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>But I kept at him; so at last he says:</td>\n",
       "      <td>2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Doan’ you ’member de house dat was float’n do...</td>\n",
       "      <td>2185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>Tom’s most well now, and got his bullet around...</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>THE END. YOURS TRULY, _HUCK FINN_.</td>\n",
       "      <td>2187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2187 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            chapter                                              chunk  \\\n",
       "0          Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
       "1          Preamble                             (Tom Sawyer’s Comrade)   \n",
       "2          Preamble                                      By Mark Twain   \n",
       "3          Preamble                                            NOTICE.   \n",
       "4          Preamble  Persons attempting to find a motive in this na...   \n",
       "...             ...                                                ...   \n",
       "2182  CHAPTER XLII.  “Nemmine why, Huck—but he ain’t comin’ back no...   \n",
       "2183  CHAPTER XLII.             But I kept at him; so at last he says:   \n",
       "2184  CHAPTER XLII.  “Doan’ you ’member de house dat was float’n do...   \n",
       "2185  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
       "2186  CHAPTER XLII.                 THE END. YOURS TRULY, _HUCK FINN_.   \n",
       "\n",
       "      chunk_order_number  \n",
       "0                      1  \n",
       "1                      2  \n",
       "2                      3  \n",
       "3                      4  \n",
       "4                      5  \n",
       "...                  ...  \n",
       "2182                2183  \n",
       "2183                2184  \n",
       "2184                2185  \n",
       "2185                2186  \n",
       "2186                2187  \n",
       "\n",
       "[2187 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the author and book title\n",
    "author_name = \"Mark Twain\"\n",
    "book_title = \"ADVENTURES OF HUCKLEBERRY FINN\"\n",
    "\n",
    "# Add the 'author' column with the specified value for all rows\n",
    "df_novel[\"author\"] = author_name\n",
    "\n",
    "# Add the 'book' column with the specified value for all rows\n",
    "df_novel[\"book\"] = book_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_order_number</th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>ADVENTURES\\nOF\\nHUCKLEBERRY FINN</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>(Tom Sawyer’s Comrade)</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>By Mark Twain</td>\n",
       "      <td>3</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>NOTICE.</td>\n",
       "      <td>4</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>Persons attempting to find a motive in this na...</td>\n",
       "      <td>5</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Nemmine why, Huck—but he ain’t comin’ back no...</td>\n",
       "      <td>2183</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>But I kept at him; so at last he says:</td>\n",
       "      <td>2184</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Doan’ you ’member de house dat was float’n do...</td>\n",
       "      <td>2185</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>Tom’s most well now, and got his bullet around...</td>\n",
       "      <td>2186</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>THE END. YOURS TRULY, _HUCK FINN_.</td>\n",
       "      <td>2187</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2187 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            chapter                                              chunk  \\\n",
       "0          Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
       "1          Preamble                             (Tom Sawyer’s Comrade)   \n",
       "2          Preamble                                      By Mark Twain   \n",
       "3          Preamble                                            NOTICE.   \n",
       "4          Preamble  Persons attempting to find a motive in this na...   \n",
       "...             ...                                                ...   \n",
       "2182  CHAPTER XLII.  “Nemmine why, Huck—but he ain’t comin’ back no...   \n",
       "2183  CHAPTER XLII.             But I kept at him; so at last he says:   \n",
       "2184  CHAPTER XLII.  “Doan’ you ’member de house dat was float’n do...   \n",
       "2185  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
       "2186  CHAPTER XLII.                 THE END. YOURS TRULY, _HUCK FINN_.   \n",
       "\n",
       "      chunk_order_number      author                            book  \n",
       "0                      1  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "1                      2  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "2                      3  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "3                      4  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "4                      5  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "...                  ...         ...                             ...  \n",
       "2182                2183  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "2183                2184  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "2184                2185  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "2185                2186  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "2186                2187  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN  \n",
       "\n",
       "[2187 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_order_number</th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>kg_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>ADVENTURES\\nOF\\nHUCKLEBERRY FINN</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>(Tom Sawyer’s Comrade)</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>By Mark Twain</td>\n",
       "      <td>3</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>NOTICE.</td>\n",
       "      <td>4</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>Persons attempting to find a motive in this na...</td>\n",
       "      <td>5</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Nemmine why, Huck—but he ain’t comin’ back no...</td>\n",
       "      <td>2183</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>But I kept at him; so at last he says:</td>\n",
       "      <td>2184</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Doan’ you ’member de house dat was float’n do...</td>\n",
       "      <td>2185</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>Tom’s most well now, and got his bullet around...</td>\n",
       "      <td>2186</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>THE END. YOURS TRULY, _HUCK FINN_.</td>\n",
       "      <td>2187</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2187 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            chapter                                              chunk  \\\n",
       "0          Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
       "1          Preamble                             (Tom Sawyer’s Comrade)   \n",
       "2          Preamble                                      By Mark Twain   \n",
       "3          Preamble                                            NOTICE.   \n",
       "4          Preamble  Persons attempting to find a motive in this na...   \n",
       "...             ...                                                ...   \n",
       "2182  CHAPTER XLII.  “Nemmine why, Huck—but he ain’t comin’ back no...   \n",
       "2183  CHAPTER XLII.             But I kept at him; so at last he says:   \n",
       "2184  CHAPTER XLII.  “Doan’ you ’member de house dat was float’n do...   \n",
       "2185  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
       "2186  CHAPTER XLII.                 THE END. YOURS TRULY, _HUCK FINN_.   \n",
       "\n",
       "      chunk_order_number      author                            book  \\\n",
       "0                      1  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "1                      2  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2                      3  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "3                      4  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "4                      5  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "...                  ...         ...                             ...   \n",
       "2182                2183  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2183                2184  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2184                2185  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2185                2186  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2186                2187  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "\n",
       "                                                kg_json  \n",
       "0     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "1     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "3     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "4     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "...                                                 ...  \n",
       "2182  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2183  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2184  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2185  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2186  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "\n",
       "[2187 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pytz\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1) Hoist your “static” system_text out of the loop so you only build it once.\n",
    "# ----------------------------------------------------------------------\n",
    "SYSTEM_TEXT = \"\"\"# Memgraph Literary Knowledge Graph Generator\n",
    "\n",
    "Generate a single structured JSON file containing both nodes and relationships for Memgraph import. This file will contain all entities and their connections in one unified format optimized for literary text analysis with the following knowledge graph schema:\n",
    "\n",
    "**Processing Context:** This prompt is designed for chunk-by-chunk processing of literary texts, where each chunk represents a segment of a chapter with specific ordering. The input data contains:\n",
    "\n",
    "- `author`: Author name\n",
    "- `book`: Book title\n",
    "- `chapter`: Chapter identifier (e.g., \"CHAPTER II\")  \n",
    "- `chunk`: Text content passage\n",
    "- `chunk_order_number`: Sequential position within the book\n",
    "- `datetime`: Current date and time in UTC at time of processing/generation\n",
    "\n",
    "## Entity Types\n",
    "\n",
    "- **Actor**: People, organizations, characters, agents\n",
    "- **Object**: Physical items, tools, documents, artifacts  \n",
    "- **Location**: Places, addresses, geographic areas\n",
    "- **Event**: Actions, incidents, occurrences, processes\n",
    "- **Intangible**: Knowledge, concepts, ideas, beliefs\n",
    "- **Book**: Literary works, novels, publications\n",
    "- **Author**: Writers, creators of literary works\n",
    "- **Chapter**: Sections or divisions within books\n",
    "- **Chunk**: Text segments or passages within chapters\n",
    "\n",
    "## Relationship Schema\n",
    "\n",
    "### Actor Relationships\n",
    "\n",
    "- `(Actor)-[INTERACTED_WITH]->(Actor)`\n",
    "- `(Actor)-[MENTIONED]->(Actor)`\n",
    "- `(Actor)-[ASSOCIATED_WITH]->(Actor)`\n",
    "- `(Actor)-[USED]->(Object)`\n",
    "- `(Actor)-[ACQUIRED]->(Object)`\n",
    "- `(Actor)-[DE_ACQUIRED]->(Object)`\n",
    "- `(Actor)-[CREATED]->(Object)`\n",
    "- `(Actor)-[DESTROYED]->(Object)`\n",
    "- `(Actor)-[MODIFIED]->(Object)`\n",
    "- `(Actor)-[ARRIVED_AT]->(Location)`\n",
    "- `(Actor)-[DEPARTED_FROM]->(Location)`\n",
    "- `(Actor)-[LOCATED_AT]->(Location)`\n",
    "- `(Actor)-[PARTICIPATED_IN]->(Event)`\n",
    "- `(Actor)-[LEARNED]->(Intangible)`\n",
    "- `(Actor)-[FORGOT]->(Intangible)`\n",
    "- `(Actor)-[CLAIMED]->(Intangible)`\n",
    "- `(Actor)-[REFUTED]->(Intangible)`\n",
    "\n",
    "### Object Relationships\n",
    "\n",
    "- `(Object)-[BELONGS_TO]->(Actor)`\n",
    "- `(Object)-[INFLUENCES]->(Actor)`\n",
    "- `(Object)-[ATTRACTS]->(Actor)`\n",
    "- `(Object)-[CONTAINS]->(Object)`\n",
    "- `(Object)-[PART_OF]->(Object)`\n",
    "- `(Object)-[CONNECTED_TO]->(Object)`\n",
    "- `(Object)-[SIMILAR_TO]->(Object)`\n",
    "- `(Object)-[LOCATED_IN]->(Location)`\n",
    "- `(Object)-[ORIGINATED_FROM]->(Location)`\n",
    "- `(Object)-[INVOLVED_IN]->(Event)`\n",
    "- `(Object)-[CAUSED]->(Event)`\n",
    "- `(Object)-[RESULTED_FROM]->(Event)`\n",
    "- `(Object)-[REPRESENTS]->(Intangible)`\n",
    "- `(Object)-[EMBODIES]->(Intangible)`\n",
    "\n",
    "### Location Relationships\n",
    "\n",
    "- `(Location)-[HOSTS]->(Actor)`\n",
    "- `(Location)-[EXCLUDES]->(Actor)`\n",
    "- `(Location)-[CONTAINS]->(Object)`\n",
    "- `(Location)-[HOUSES]->(Object)`\n",
    "- `(Location)-[CONTAINS]->(Location)`\n",
    "- `(Location)-[ADJACENT_TO]->(Location)`\n",
    "- `(Location)-[PART_OF]->(Location)`\n",
    "- `(Location)-[CONNECTED_TO]->(Location)`\n",
    "- `(Location)-[HOSTED]->(Event)`\n",
    "- `(Location)-[WITNESSED]->(Event)`\n",
    "- `(Location)-[ASSOCIATED_WITH]->(Intangible)`\n",
    "- `(Location)-[SYMBOLIZES]->(Intangible)`\n",
    "\n",
    "### Event Relationships\n",
    "\n",
    "- `(Event)-[AFFECTED]->(Actor)`\n",
    "- `(Event)-[CAUSED_BY]->(Actor)`\n",
    "- `(Event)-[INVOLVED]->(Object)`\n",
    "- `(Event)-[PRODUCED]->(Object)`\n",
    "- `(Event)-[CONSUMED]->(Object)`\n",
    "- `(Event)-[OCCURRED_AT]->(Location)`\n",
    "- `(Event)-[MOVED_FROM]->(Location)`\n",
    "- `(Event)-[MOVED_TO]->(Location)`\n",
    "- `(Event)-[PRECEDED]->(Event)`\n",
    "- `(Event)-[FOLLOWED]->(Event)`\n",
    "- `(Event)-[CAUSED]->(Event)`\n",
    "- `(Event)-[CONCURRENT_WITH]->(Event)`\n",
    "- `(Event)-[REVEALED]->(Intangible)`\n",
    "- `(Event)-[DEMONSTRATED]->(Intangible)`\n",
    "- `(Event)-[RESULTED_IN]->(Intangible)`\n",
    "\n",
    "### Intangible Relationships\n",
    "\n",
    "- `(Intangible)-[INFLUENCED]->(Actor)`\n",
    "- `(Intangible)-[POSSESSED_BY]->(Actor)`\n",
    "- `(Intangible)-[KNOWN_BY]->(Actor)`\n",
    "- `(Intangible)-[APPLIED_TO]->(Object)`\n",
    "- `(Intangible)-[MANIFESTED_IN]->(Object)`\n",
    "- `(Intangible)-[ORIGINATED_FROM]->(Location)`\n",
    "- `(Intangible)-[ASSOCIATED_WITH]->(Location)`\n",
    "- `(Intangible)-[DEMONSTRATED_IN]->(Event)`\n",
    "- `(Intangible)-[REVEALED_BY]->(Event)`\n",
    "- `(Intangible)-[CONTRADICTS]->(Intangible)`\n",
    "- `(Intangible)-[SUPPORTS]->(Intangible)`\n",
    "- `(Intangible)-[DERIVED_FROM]->(Intangible)`\n",
    "- `(Intangible)-[RELATED_TO]->(Intangible)`\n",
    "\n",
    "### Book Relationships\n",
    "\n",
    "- `(Book)-[WRITTEN_BY]->(Author)`\n",
    "- `(Book)-[CONTAINS]->(Chapter)`\n",
    "- `(Book)-[FEATURES]->(Actor)`\n",
    "- `(Book)-[MENTIONS]->(Object)`\n",
    "- `(Book)-[SET_IN]->(Location)`\n",
    "- `(Book)-[DESCRIBES]->(Event)`\n",
    "- `(Book)-[EXPLORES]->(Intangible)`\n",
    "- `(Book)-[PUBLISHED_AT]->(Location)`\n",
    "- `(Book)-[PUBLISHED_IN]->(Event)`\n",
    "\n",
    "### Author Relationships\n",
    "\n",
    "- `(Author)-[WROTE]->(Book)`\n",
    "- `(Author)-[CREATED]->(Object)`\n",
    "- `(Author)-[LIVED_IN]->(Location)`\n",
    "- `(Author)-[BORN_IN]->(Location)`\n",
    "- `(Author)-[PARTICIPATED_IN]->(Event)`\n",
    "- `(Author)-[INFLUENCED_BY]->(Intangible)`\n",
    "- `(Author)-[INFLUENCED]->(Actor)`\n",
    "- `(Author)-[CONTEMPORARY_OF]->(Author)`\n",
    "- `(Author)-[INSPIRED_BY]->(Author)`\n",
    "\n",
    "### Chapter Relationships\n",
    "\n",
    "- `(Chapter)-[PART_OF]->(Book)`\n",
    "- `(Chapter)-[CONTAINS]->(Chunk)`\n",
    "- `(Chapter)-[PRECEDED_BY]->(Chapter)`\n",
    "- `(Chapter)-[FOLLOWED_BY]->(Chapter)`\n",
    "- `(Chapter)-[FEATURES]->(Actor)`\n",
    "- `(Chapter)-[MENTIONS]->(Object)`\n",
    "- `(Chapter)-[SET_IN]->(Location)`\n",
    "- `(Chapter)-[DESCRIBES]->(Event)`\n",
    "- `(Chapter)-[CONVEYS]->(Intangible)`\n",
    "\n",
    "### Chunk Relationships\n",
    "\n",
    "- `(Chunk)-[PART_OF]->(Chapter)`\n",
    "- `(Chunk)-[PRECEDED_BY]->(Chunk)`\n",
    "- `(Chunk)-[FOLLOWED_BY]->(Chunk)`\n",
    "- `(Chunk)-[MENTIONS]->(Actor)`\n",
    "- `(Chunk)-[REFERENCES]->(Object)`\n",
    "- `(Chunk)-[DESCRIBES]->(Location)`\n",
    "- `(Chunk)-[DEPICTS]->(Event)`\n",
    "- `(Chunk)-[CONVEYS]->(Intangible)`\n",
    "- `(Chunk)-[CONTAINS_DIALOGUE_BY]->(Actor)`\n",
    "\n",
    "## Output Requirements\n",
    "\n",
    "**Generate a single structured JSON file containing both nodes and relationships.**\n",
    "\n",
    "### JSON Structure: **knowledge_graph.json**\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"metadata\": {{\n",
    "    \"generated_at\": \"{timestamp}\",\n",
    "    \"total_nodes\": 85,\n",
    "    \"total_relationships\": 157,\n",
    "    \"entity_types\": [\"Actor\", \"Object\", \"Location\", \"Event\", \"Intangible\", \"Book\", \"Author\", \"Chapter\", \"Chunk\"]\n",
    "  }},\n",
    "  \"nodes\": [\n",
    "    {{\n",
    "      \"id\": 1,\n",
    "      \"label\": \"Book\",\n",
    "      \"name\": \"The Adventures of Huckleberry Finn\",\n",
    "      \"description\": \"Classic American novel by Mark Twain\",\n",
    "      \"properties\": {{\n",
    "        \"genre\": \"Adventure Fiction\",\n",
    "        \"publication_year\": 1884\n",
    "      }},\n",
    "      \"timestamp\": \"{timestamp}\"\n",
    "    }}\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {{\n",
    "      \"start_id\": 1,\n",
    "      \"end_id\": 2,\n",
    "      \"relationship_type\": \"WRITTEN_BY\",\n",
    "      \"weight\": 1.0,\n",
    "      \"properties\": {{\n",
    "        \"relationship_strength\": \"primary\"\n",
    "      }},\n",
    "      \"timestamp\": \"{timestamp}\"\n",
    "    }}\n",
    "  ]\n",
    "}}```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2) Your generate function can remain exactly as before; it still expects `row[...]`\n",
    "# ----------------------------------------------------------------------\n",
    "async def generate_chunk_json_simple(client: genai.Client, row: pd.Series) -> str:\n",
    "    now_iso = datetime.datetime.now(pytz.UTC).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    model = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "    user_text = (\n",
    "        f\"Author: {row['author']}\\n\"\n",
    "        f\"Book: {row['book']}\\n\"\n",
    "        f\"Chapter: {row['chapter']}\\n\"\n",
    "        f\"chunk_order_number: {row['chunk_order_number']}\\n\"\n",
    "        f\"Chunk: {row['chunk']}\\n\"\n",
    "        f\"Datetime: {now_iso}\\n\"\n",
    "    )\n",
    "\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part.from_text(text=SYSTEM_TEXT + \"\\n\" + user_text)],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        # (You can optionally set max_output_token_count here if you want to cap response length)\n",
    "        # max_output_token_count=256\n",
    "    )\n",
    "\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3) New “process_dataframe_concurrent” that:\n",
    "#    • launches all tasks immediately (no asyncio.sleep)\n",
    "#    • uses a Semaphore(50) to cap concurrency at 50\n",
    "#    • iterates with df.iterrows() so row[...] works\n",
    "# ----------------------------------------------------------------------\n",
    "async def process_dataframe_concurrent(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "    semaphore = asyncio.Semaphore(50)  # cap at 50 in-flight calls\n",
    "\n",
    "    async def generate_with_semaphore(row: pd.Series) -> str:\n",
    "        async with semaphore:\n",
    "            try:\n",
    "                return await generate_chunk_json_simple(client, row)\n",
    "            except Exception as e:\n",
    "                return f'{{\"error\": \"{e}\"}}'\n",
    "\n",
    "    # Kick off one task per row, but each call to generate will wait its turn under the semaphore\n",
    "    tasks = [\n",
    "        asyncio.create_task(generate_with_semaphore(row)) for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    df2 = df.copy()\n",
    "    df2[\"kg_json\"] = results\n",
    "    return df2\n",
    "\n",
    "\n",
    "# ===== JUPYTER NOTEBOOK USAGE =====\n",
    "# In your Jupyter cell, just call:\n",
    "# updated_df = await process_dataframe_concurrent(df_novel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then in Jupyter:\n",
    "updated_df = await process_dataframe_concurrent(df_novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metadata': {'generated_at': '2024-02-29T12:00:00Z',\n",
       "   'total_nodes': 6,\n",
       "   'total_relationships': 5,\n",
       "   'entity_types': ['Actor', 'Object', 'Book', 'Author', 'Chapter', 'Chunk']},\n",
       "  'nodes': [{'id': 1,\n",
       "    'label': 'Author',\n",
       "    'name': 'Mark Twain',\n",
       "    'description': 'Author of ADVENTURES OF HUCKLEBERRY FINN',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'id': 2,\n",
       "    'label': 'Book',\n",
       "    'name': 'ADVENTURES OF HUCKLEBERRY FINN',\n",
       "    'description': 'Book by Mark Twain',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'id': 3,\n",
       "    'label': 'Chapter',\n",
       "    'name': 'CHAPTER I.',\n",
       "    'description': 'Chapter 1 of ADVENTURES OF HUCKLEBERRY FINN',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'id': 4,\n",
       "    'label': 'Chunk',\n",
       "    'name': 'Chunk 16',\n",
       "    'description': 'Text passage from CHAPTER I.',\n",
       "    'properties': {'chunk_order_number': 16},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'id': 5,\n",
       "    'label': 'Actor',\n",
       "    'name': 'Moses',\n",
       "    'description': 'Biblical figure',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'id': 6,\n",
       "    'label': 'Object',\n",
       "    'name': 'book',\n",
       "    'description': 'a book',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'}],\n",
       "  'relationships': [{'start_id': 2,\n",
       "    'end_id': 1,\n",
       "    'relationship_type': 'WRITTEN_BY',\n",
       "    'weight': 1.0,\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'start_id': 3,\n",
       "    'end_id': 2,\n",
       "    'relationship_type': 'PART_OF',\n",
       "    'weight': 1.0,\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'start_id': 4,\n",
       "    'end_id': 3,\n",
       "    'relationship_type': 'PART_OF',\n",
       "    'weight': 1.0,\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'start_id': 4,\n",
       "    'end_id': 5,\n",
       "    'relationship_type': 'MENTIONS',\n",
       "    'weight': 1.0,\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'},\n",
       "   {'start_id': 4,\n",
       "    'end_id': 6,\n",
       "    'relationship_type': 'REFERENCES',\n",
       "    'weight': 1.0,\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:36:15Z'}]}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first row's kg_json as multi-line text\n",
    "import json\n",
    "json.loads(updated_df[\"kg_json\"].iloc[15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_order_number</th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>kg_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>ADVENTURES\\nOF\\nHUCKLEBERRY FINN</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>(Tom Sawyer’s Comrade)</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>By Mark Twain</td>\n",
       "      <td>3</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>NOTICE.</td>\n",
       "      <td>4</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>Persons attempting to find a motive in this na...</td>\n",
       "      <td>5</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Nemmine why, Huck—but he ain’t comin’ back no...</td>\n",
       "      <td>2183</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>But I kept at him; so at last he says:</td>\n",
       "      <td>2184</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>“Doan’ you ’member de house dat was float’n do...</td>\n",
       "      <td>2185</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>Tom’s most well now, and got his bullet around...</td>\n",
       "      <td>2186</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>THE END. YOURS TRULY, _HUCK FINN_.</td>\n",
       "      <td>2187</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>[\\n  {\\n    \"metadata\": {\\n      \"generated_at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2187 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            chapter                                              chunk  \\\n",
       "0          Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
       "1          Preamble                             (Tom Sawyer’s Comrade)   \n",
       "2          Preamble                                      By Mark Twain   \n",
       "3          Preamble                                            NOTICE.   \n",
       "4          Preamble  Persons attempting to find a motive in this na...   \n",
       "...             ...                                                ...   \n",
       "2182  CHAPTER XLII.  “Nemmine why, Huck—but he ain’t comin’ back no...   \n",
       "2183  CHAPTER XLII.             But I kept at him; so at last he says:   \n",
       "2184  CHAPTER XLII.  “Doan’ you ’member de house dat was float’n do...   \n",
       "2185  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
       "2186  CHAPTER XLII.                 THE END. YOURS TRULY, _HUCK FINN_.   \n",
       "\n",
       "      chunk_order_number      author                            book  \\\n",
       "0                      1  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "1                      2  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2                      3  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "3                      4  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "4                      5  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "...                  ...         ...                             ...   \n",
       "2182                2183  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2183                2184  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2184                2185  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2185                2186  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2186                2187  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "\n",
       "                                                kg_json  \n",
       "0     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "1     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "3     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "4     [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "...                                                 ...  \n",
       "2182  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2183  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2184  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2185  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "2186  [\\n  {\\n    \"metadata\": {\\n      \"generated_at...  \n",
       "\n",
       "[2187 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to UUID Mapping:\n",
      "--------------------------------------------------\n",
      "Chunk 1:\n",
      "  1 -> f1912760-8749-51e0-908a-70e6ce194ca4 (Author: Mark Twain)\n",
      "  2 -> f32bd123-42c0-5ba0-a943-797dc52d7e1a (Book: ADVENTURES OF HUCKLEBERRY FINN)\n",
      "  3 -> d3cdfa5b-eeb7-59fb-afef-09fee86196ea (Chapter: CHAPTER I.)\n",
      "  4 -> 56d34d02-4232-5a49-900e-363974bc6986 (Chunk: Chunk 16)\n",
      "  5 -> 169b7f92-46f8-587d-8944-68f47c0f918a (Actor: Moses)\n",
      "  6 -> c1e8b72b-ee1d-5fe8-8e33-2b28bee79ed4 (Object: book)\n",
      "\n",
      "Converted Data:\n",
      "==================================================\n",
      "[\n",
      "  {\n",
      "    \"metadata\": {\n",
      "      \"generated_at\": \"2024-02-29T12:00:00Z\",\n",
      "      \"total_nodes\": 6,\n",
      "      \"total_relationships\": 5,\n",
      "      \"entity_types\": [\n",
      "        \"Actor\",\n",
      "        \"Object\",\n",
      "        \"Book\",\n",
      "        \"Author\",\n",
      "        \"Chapter\",\n",
      "        \"Chunk\"\n",
      "      ]\n",
      "    },\n",
      "    \"nodes\": [\n",
      "      {\n",
      "        \"id\": \"f1912760-8749-51e0-908a-70e6ce194ca4\",\n",
      "        \"label\": \"Author\",\n",
      "        \"name\": \"Mark Twain\",\n",
      "        \"description\": \"Author of ADVENTURES OF HUCKLEBERRY FINN\",\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"f32bd123-42c0-5ba0-a943-797dc52d7e1a\",\n",
      "        \"label\": \"Book\",\n",
      "        \"name\": \"ADVENTURES OF HUCKLEBERRY FINN\",\n",
      "        \"description\": \"Book by Mark Twain\",\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"d3cdfa5b-eeb7-59fb-afef-09fee86196ea\",\n",
      "        \"label\": \"Chapter\",\n",
      "        \"name\": \"CHAPTER I.\",\n",
      "        \"description\": \"Chapter 1 of ADVENTURES OF HUCKLEBERRY FINN\",\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"56d34d02-4232-5a49-900e-363974bc6986\",\n",
      "        \"label\": \"Chunk\",\n",
      "        \"name\": \"Chunk 16\",\n",
      "        \"description\": \"Text passage from CHAPTER I.\",\n",
      "        \"properties\": {\n",
      "          \"chunk_order_number\": 16\n",
      "        },\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"169b7f92-46f8-587d-8944-68f47c0f918a\",\n",
      "        \"label\": \"Actor\",\n",
      "        \"name\": \"Moses\",\n",
      "        \"description\": \"Biblical figure\",\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"id\": \"c1e8b72b-ee1d-5fe8-8e33-2b28bee79ed4\",\n",
      "        \"label\": \"Object\",\n",
      "        \"name\": \"book\",\n",
      "        \"description\": \"a book\",\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      }\n",
      "    ],\n",
      "    \"relationships\": [\n",
      "      {\n",
      "        \"start_id\": \"f32bd123-42c0-5ba0-a943-797dc52d7e1a\",\n",
      "        \"end_id\": \"f1912760-8749-51e0-908a-70e6ce194ca4\",\n",
      "        \"relationship_type\": \"WRITTEN_BY\",\n",
      "        \"weight\": 1.0,\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"start_id\": \"d3cdfa5b-eeb7-59fb-afef-09fee86196ea\",\n",
      "        \"end_id\": \"f32bd123-42c0-5ba0-a943-797dc52d7e1a\",\n",
      "        \"relationship_type\": \"PART_OF\",\n",
      "        \"weight\": 1.0,\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"start_id\": \"56d34d02-4232-5a49-900e-363974bc6986\",\n",
      "        \"end_id\": \"d3cdfa5b-eeb7-59fb-afef-09fee86196ea\",\n",
      "        \"relationship_type\": \"PART_OF\",\n",
      "        \"weight\": 1.0,\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"start_id\": \"56d34d02-4232-5a49-900e-363974bc6986\",\n",
      "        \"end_id\": \"169b7f92-46f8-587d-8944-68f47c0f918a\",\n",
      "        \"relationship_type\": \"MENTIONS\",\n",
      "        \"weight\": 1.0,\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      },\n",
      "      {\n",
      "        \"start_id\": \"56d34d02-4232-5a49-900e-363974bc6986\",\n",
      "        \"end_id\": \"c1e8b72b-ee1d-5fe8-8e33-2b28bee79ed4\",\n",
      "        \"relationship_type\": \"REFERENCES\",\n",
      "        \"weight\": 1.0,\n",
      "        \"properties\": {},\n",
      "        \"timestamp\": \"2025-06-01T16:36:15Z\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "Testing consistency - Mark Twain should always get the same UUID:\n",
      "UUID 1: f1912760-8749-51e0-908a-70e6ce194ca4\n",
      "UUID 2: f1912760-8749-51e0-908a-70e6ce194ca4\n",
      "Same UUID: True\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "# import hashlib\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "\n",
    "def generate_deterministic_uuid(label: str, name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a deterministic UUID based on label and name.\n",
    "    Uses UUID5 with a custom namespace for consistency.\n",
    "    \"\"\"\n",
    "    # Create a custom namespace UUID for our application\n",
    "    namespace = uuid.UUID(\"6ba7b810-9dad-11d1-80b4-00c04fd430c8\")  # Standard namespace\n",
    "\n",
    "    # Combine label and name for the hash input\n",
    "    combined_string = f\"{label}:{name}\"\n",
    "\n",
    "    # Generate deterministic UUID5\n",
    "    return str(uuid.uuid5(namespace, combined_string))\n",
    "\n",
    "\n",
    "def convert_nodes_to_uuid(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert node IDs to UUIDs and update relationships accordingly.\n",
    "    \"\"\"\n",
    "    converted_data = []\n",
    "\n",
    "    for chunk in data:\n",
    "        # Create mapping from old ID to new UUID\n",
    "        id_to_uuid_mapping = {}\n",
    "\n",
    "        # Process nodes and create UUID mapping\n",
    "        new_nodes = []\n",
    "        for node in chunk[\"nodes\"]:\n",
    "            old_id = node[\"id\"]\n",
    "            new_uuid = generate_deterministic_uuid(node[\"label\"], node[\"name\"])\n",
    "\n",
    "            # Store mapping for relationship updates\n",
    "            id_to_uuid_mapping[old_id] = new_uuid\n",
    "\n",
    "            # Create new node with UUID\n",
    "            new_node = node.copy()\n",
    "            new_node[\"id\"] = new_uuid\n",
    "            new_nodes.append(new_node)\n",
    "\n",
    "        # Process relationships with updated IDs\n",
    "        new_relationships = []\n",
    "        for rel in chunk[\"relationships\"]:\n",
    "            new_rel = rel.copy()\n",
    "            new_rel[\"start_id\"] = id_to_uuid_mapping[rel[\"start_id\"]]\n",
    "            new_rel[\"end_id\"] = id_to_uuid_mapping[rel[\"end_id\"]]\n",
    "            new_relationships.append(new_rel)\n",
    "\n",
    "        # Create new chunk with updated data\n",
    "        new_chunk = chunk.copy()\n",
    "        new_chunk[\"nodes\"] = new_nodes\n",
    "        new_chunk[\"relationships\"] = new_relationships\n",
    "\n",
    "        converted_data.append(new_chunk)\n",
    "\n",
    "    return converted_data\n",
    "\n",
    "\n",
    "def print_uuid_mapping(data: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Print the mapping between old IDs and new UUIDs for verification.\n",
    "    \"\"\"\n",
    "    print(\"ID to UUID Mapping:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for chunk_idx, chunk in enumerate(data):\n",
    "        print(f\"Chunk {chunk_idx + 1}:\")\n",
    "        for node in chunk[\"nodes\"]:\n",
    "            label = node[\"label\"]\n",
    "            name = node[\"name\"]\n",
    "            old_id = node[\"id\"]\n",
    "            new_uuid = generate_deterministic_uuid(label, name)\n",
    "            print(f\"  {old_id} -> {new_uuid} ({label}: {name})\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# Example usage with your sample data\n",
    "sample_data = [\n",
    "    {\n",
    "        \"metadata\": {\n",
    "            \"generated_at\": \"2024-02-29T12:00:00Z\",\n",
    "            \"total_nodes\": 6,\n",
    "            \"total_relationships\": 5,\n",
    "            \"entity_types\": [\"Actor\", \"Object\", \"Book\", \"Author\", \"Chapter\", \"Chunk\"],\n",
    "        },\n",
    "        \"nodes\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"label\": \"Author\",\n",
    "                \"name\": \"Mark Twain\",\n",
    "                \"description\": \"Author of ADVENTURES OF HUCKLEBERRY FINN\",\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"label\": \"Book\",\n",
    "                \"name\": \"ADVENTURES OF HUCKLEBERRY FINN\",\n",
    "                \"description\": \"Book by Mark Twain\",\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"label\": \"Chapter\",\n",
    "                \"name\": \"CHAPTER I.\",\n",
    "                \"description\": \"Chapter 1 of ADVENTURES OF HUCKLEBERRY FINN\",\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"label\": \"Chunk\",\n",
    "                \"name\": \"Chunk 16\",\n",
    "                \"description\": \"Text passage from CHAPTER I.\",\n",
    "                \"properties\": {\"chunk_order_number\": 16},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"label\": \"Actor\",\n",
    "                \"name\": \"Moses\",\n",
    "                \"description\": \"Biblical figure\",\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"label\": \"Object\",\n",
    "                \"name\": \"book\",\n",
    "                \"description\": \"a book\",\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "        ],\n",
    "        \"relationships\": [\n",
    "            {\n",
    "                \"start_id\": 2,\n",
    "                \"end_id\": 1,\n",
    "                \"relationship_type\": \"WRITTEN_BY\",\n",
    "                \"weight\": 1.0,\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"start_id\": 3,\n",
    "                \"end_id\": 2,\n",
    "                \"relationship_type\": \"PART_OF\",\n",
    "                \"weight\": 1.0,\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"start_id\": 4,\n",
    "                \"end_id\": 3,\n",
    "                \"relationship_type\": \"PART_OF\",\n",
    "                \"weight\": 1.0,\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"start_id\": 4,\n",
    "                \"end_id\": 5,\n",
    "                \"relationship_type\": \"MENTIONS\",\n",
    "                \"weight\": 1.0,\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "            {\n",
    "                \"start_id\": 4,\n",
    "                \"end_id\": 6,\n",
    "                \"relationship_type\": \"REFERENCES\",\n",
    "                \"weight\": 1.0,\n",
    "                \"properties\": {},\n",
    "                \"timestamp\": \"2025-06-01T16:36:15Z\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Show the mapping for verification\n",
    "    print_uuid_mapping(sample_data)\n",
    "\n",
    "    # Convert the data\n",
    "    converted_data = convert_nodes_to_uuid(sample_data)\n",
    "\n",
    "    # Print the converted data\n",
    "    print(\"Converted Data:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(json.dumps(converted_data, indent=2))\n",
    "\n",
    "    # Verify that the same entity gets the same UUID consistently\n",
    "    print(\"\\nTesting consistency - Mark Twain should always get the same UUID:\")\n",
    "    uuid1 = generate_deterministic_uuid(\"Author\", \"Mark Twain\")\n",
    "    uuid2 = generate_deterministic_uuid(\"Author\", \"Mark Twain\")\n",
    "    print(f\"UUID 1: {uuid1}\")\n",
    "    print(f\"UUID 2: {uuid2}\")\n",
    "    print(f\"Same UUID: {uuid1 == uuid2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2187 entries, 0 to 2186\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   chapter             2187 non-null   object\n",
      " 1   chunk               2187 non-null   object\n",
      " 2   chunk_order_number  2187 non-null   int64 \n",
      " 3   author              2187 non-null   object\n",
      " 4   book                2187 non-null   object\n",
      " 5   kg_json             2187 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 102.6+ KB\n"
     ]
    }
   ],
   "source": [
    "updated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas pyarrow\n",
    "\n",
    "updated_df.to_feather(\"huckleberry_finn_kg_data_unnormalized.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced functions loaded! Now handles missing names and JSON errors.\n",
      "📞 Call: processed_df, error_stats = process_updated_df_complete(updated_df)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def generate_deterministic_uuid(label: str, name: Optional[str] = None, node_id: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a deterministic UUID based on label and name.\n",
    "    Falls back to using label + node_id if name is missing.\n",
    "    \"\"\"\n",
    "    namespace = uuid.UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')\n",
    "    \n",
    "    if name:\n",
    "        combined_string = f\"{label}:{name}\"\n",
    "    elif node_id is not None:\n",
    "        # Fallback: use label + original node_id if name is missing\n",
    "        combined_string = f\"{label}:node_{node_id}\"\n",
    "    else:\n",
    "        # Last resort: just use label + random component\n",
    "        combined_string = f\"{label}:unnamed_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    return str(uuid.uuid5(namespace, combined_string))\n",
    "\n",
    "def clean_json_string(json_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean common JSON issues like invalid escape sequences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fix common escape sequence issues\n",
    "        # Replace invalid escapes like \\' and \\\" that aren't properly escaped\n",
    "        cleaned = re.sub(r'\\\\(?![\"\\\\/bfnrt]|u[0-9a-fA-F]{4})', r'\\\\\\\\', json_str)\n",
    "        return cleaned\n",
    "    except Exception:\n",
    "        return json_str\n",
    "\n",
    "def convert_nodes_to_uuid(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert node IDs to UUIDs and update relationships accordingly.\n",
    "    Handles missing 'name' fields gracefully.\n",
    "    \"\"\"\n",
    "    converted_data = []\n",
    "    \n",
    "    for chunk in data:\n",
    "        # Create mapping from old ID to new UUID\n",
    "        id_to_uuid_mapping = {}\n",
    "        \n",
    "        # Process nodes and create UUID mapping\n",
    "        new_nodes = []\n",
    "        for node in chunk.get('nodes', []):\n",
    "            old_id = node.get('id')\n",
    "            label = node.get('label', 'Unknown')\n",
    "            name = node.get('name')  # This might be None/missing\n",
    "            \n",
    "            # Generate UUID with fallback handling\n",
    "            new_uuid = generate_deterministic_uuid(label, name, old_id)\n",
    "            \n",
    "            # Store mapping for relationship updates\n",
    "            if old_id is not None:\n",
    "                id_to_uuid_mapping[old_id] = new_uuid\n",
    "            \n",
    "            # Create new node with UUID\n",
    "            new_node = node.copy()\n",
    "            new_node['id'] = new_uuid\n",
    "            \n",
    "            # Add name if missing (for consistency)\n",
    "            if 'name' not in new_node or new_node['name'] is None:\n",
    "                new_node['name'] = f\"{label}_{old_id}\" if old_id else f\"{label}_unnamed\"\n",
    "            \n",
    "            new_nodes.append(new_node)\n",
    "        \n",
    "        # Process relationships with updated IDs\n",
    "        new_relationships = []\n",
    "        for rel in chunk.get('relationships', []):\n",
    "            new_rel = rel.copy()\n",
    "            start_id = rel.get('start_id')\n",
    "            end_id = rel.get('end_id')\n",
    "            \n",
    "            # Only update if we have the mapping\n",
    "            if start_id in id_to_uuid_mapping:\n",
    "                new_rel['start_id'] = id_to_uuid_mapping[start_id]\n",
    "            if end_id in id_to_uuid_mapping:\n",
    "                new_rel['end_id'] = id_to_uuid_mapping[end_id]\n",
    "                \n",
    "            new_relationships.append(new_rel)\n",
    "        \n",
    "        # Create new chunk with updated data\n",
    "        new_chunk = chunk.copy()\n",
    "        new_chunk['nodes'] = new_nodes\n",
    "        new_chunk['relationships'] = new_relationships\n",
    "        \n",
    "        converted_data.append(new_chunk)\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "def process_kg_json_row(kg_json_str: str, row_index: int) -> tuple[str, bool, str]:\n",
    "    \"\"\"\n",
    "    Process a single row's kg_json string and convert to UUID format.\n",
    "    Returns: (processed_json, success, error_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the JSON string first\n",
    "        cleaned_json = clean_json_string(kg_json_str)\n",
    "        \n",
    "        # Parse the JSON string\n",
    "        kg_data = json.loads(cleaned_json)\n",
    "        \n",
    "        # Handle both single chunk and list of chunks\n",
    "        if isinstance(kg_data, dict):\n",
    "            kg_list = [kg_data]\n",
    "        elif isinstance(kg_data, list):\n",
    "            kg_list = kg_data\n",
    "        else:\n",
    "            return kg_json_str, False, f\"Unexpected data type: {type(kg_data)}\"\n",
    "        \n",
    "        # Convert to UUID format\n",
    "        converted_data = convert_nodes_to_uuid(kg_list)\n",
    "        \n",
    "        # Return the same format as input\n",
    "        if isinstance(kg_data, dict):\n",
    "            result = converted_data[0]\n",
    "        else:\n",
    "            result = converted_data\n",
    "        \n",
    "        return json.dumps(result), True, \"\"\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return kg_json_str, False, f\"JSON decode error: {str(e)}\"\n",
    "    except KeyError as e:\n",
    "        return kg_json_str, False, f\"Missing key: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return kg_json_str, False, f\"Processing error: {str(e)}\"\n",
    "\n",
    "def analyze_errors(df: pd.DataFrame, sample_error_rows: List[int] = None) -> None:\n",
    "    \"\"\"\n",
    "    Analyze the problematic rows to understand the issues better.\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 Analyzing error patterns...\")\n",
    "    \n",
    "    if sample_error_rows is None:\n",
    "        sample_error_rows = []\n",
    "    \n",
    "    for row_idx in sample_error_rows[:3]:  # Analyze first 3 error rows\n",
    "        try:\n",
    "            print(f\"\\n--- Row {row_idx} Analysis ---\")\n",
    "            kg_json = df.iloc[row_idx]['kg_json']\n",
    "            \n",
    "            # Try to parse and show structure\n",
    "            try:\n",
    "                cleaned = clean_json_string(kg_json)\n",
    "                data = json.loads(cleaned)\n",
    "                \n",
    "                if isinstance(data, list) and len(data) > 0:\n",
    "                    data = data[0]  # Look at first chunk\n",
    "                \n",
    "                if isinstance(data, dict):\n",
    "                    nodes = data.get('nodes', [])\n",
    "                    print(f\"  Nodes found: {len(nodes)}\")\n",
    "                    \n",
    "                    for i, node in enumerate(nodes[:2]):  # Show first 2 nodes\n",
    "                        print(f\"    Node {i}: {node}\")\n",
    "                        missing_fields = []\n",
    "                        if 'name' not in node or node.get('name') is None:\n",
    "                            missing_fields.append('name')\n",
    "                        if 'label' not in node:\n",
    "                            missing_fields.append('label')\n",
    "                        if 'id' not in node:\n",
    "                            missing_fields.append('id')\n",
    "                        if missing_fields:\n",
    "                            print(f\"      Missing fields: {missing_fields}\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"  Analysis error: {e}\")\n",
    "                print(f\"  First 200 chars: {kg_json[:200]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Row {row_idx}: Could not analyze - {e}\")\n",
    "\n",
    "def process_dataframe_kg_json(df: pd.DataFrame, batch_size: int = 100) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Process all rows in the dataframe to convert kg_json to UUID format.\n",
    "    Returns: (processed_dataframe, error_stats)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting processing of {len(df)} rows...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Track errors and statistics\n",
    "    error_stats = {\n",
    "        'total_errors': 0,\n",
    "        'json_decode_errors': 0,\n",
    "        'missing_name_errors': 0,\n",
    "        'other_errors': 0,\n",
    "        'error_rows': []\n",
    "    }\n",
    "    \n",
    "    converted_json = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        processed_json, success, error_msg = process_kg_json_row(row['kg_json'], idx)\n",
    "        converted_json.append(processed_json)\n",
    "        \n",
    "        if not success:\n",
    "            error_stats['total_errors'] += 1\n",
    "            error_stats['error_rows'].append(idx)\n",
    "            \n",
    "            if 'JSON decode error' in error_msg:\n",
    "                error_stats['json_decode_errors'] += 1\n",
    "            elif 'name' in error_msg.lower():\n",
    "                error_stats['missing_name_errors'] += 1\n",
    "            else:\n",
    "                error_stats['other_errors'] += 1\n",
    "            \n",
    "            logger.error(f\"Row {idx}: {error_msg}\")\n",
    "        \n",
    "        # Log progress\n",
    "        if (idx + 1) % batch_size == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(df)} rows... (Errors so far: {error_stats['total_errors']})\")\n",
    "    \n",
    "    # Update the dataframe\n",
    "    result_df['kg_json'] = converted_json\n",
    "    \n",
    "    print(f\"Processing complete! Processed {len(df)} rows with {error_stats['total_errors']} errors.\")\n",
    "    \n",
    "    return result_df, error_stats\n",
    "\n",
    "def quick_process_updated_df(updated_df):\n",
    "    \"\"\"\n",
    "    Quick processing function with detailed error reporting.\n",
    "    \"\"\"\n",
    "    print(f\"📊 Processing dataframe with {len(updated_df)} rows...\")\n",
    "    print(f\"📋 Columns: {list(updated_df.columns)}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process the kg_json column\n",
    "    processed_df, error_stats = process_dataframe_kg_json(updated_df, batch_size=200)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(f\"⏱️  Processing completed in {processing_time:.2f} seconds\")\n",
    "    print(f\"📈 Average time per row: {processing_time/len(updated_df)*1000:.2f} ms\")\n",
    "    \n",
    "    # Show detailed error statistics\n",
    "    print(f\"\\n📊 Error Statistics:\")\n",
    "    print(f\"   Total errors: {error_stats['total_errors']}\")\n",
    "    print(f\"   JSON decode errors: {error_stats['json_decode_errors']}\")\n",
    "    print(f\"   Missing name errors: {error_stats['missing_name_errors']}\")\n",
    "    print(f\"   Other errors: {error_stats['other_errors']}\")\n",
    "    print(f\"   Success rate: {((len(updated_df) - error_stats['total_errors']) / len(updated_df) * 100):.1f}%\")\n",
    "    \n",
    "    if error_stats['error_rows']:\n",
    "        analyze_errors(updated_df, error_stats['error_rows'])\n",
    "    \n",
    "    return processed_df, error_stats\n",
    "\n",
    "def validate_conversion(original_df: pd.DataFrame, converted_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Quick validation of the conversion.\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 Validating conversion...\")\n",
    "    \n",
    "    try:\n",
    "        # Check first row\n",
    "        original = json.loads(original_df['kg_json'].iloc[0])\n",
    "        converted = json.loads(converted_df['kg_json'].iloc[0])\n",
    "        \n",
    "        # Get nodes from the data structure\n",
    "        if isinstance(original, dict):\n",
    "            orig_nodes = original.get('nodes', [])\n",
    "            conv_nodes = converted.get('nodes', [])\n",
    "        else:\n",
    "            orig_nodes = original[0].get('nodes', []) if original else []\n",
    "            conv_nodes = converted[0].get('nodes', []) if converted else []\n",
    "        \n",
    "        print(f\"   Original nodes: {len(orig_nodes)}\")\n",
    "        print(f\"   Converted nodes: {len(conv_nodes)}\")\n",
    "        \n",
    "        if len(orig_nodes) > 0 and len(conv_nodes) > 0:\n",
    "            print(f\"   Sample original ID: {orig_nodes[0]['id']}\")\n",
    "            print(f\"   Sample converted ID: {conv_nodes[0]['id']}\")\n",
    "            \n",
    "            # Verify UUID format\n",
    "            try:\n",
    "                uuid.UUID(conv_nodes[0]['id'])\n",
    "                print(f\"   ✅ Valid UUID format\")\n",
    "            except ValueError:\n",
    "                print(f\"   ❌ Invalid UUID format\")\n",
    "        \n",
    "        # Check for consistent name handling\n",
    "        if len(conv_nodes) > 0:\n",
    "            has_name = 'name' in conv_nodes[0] and conv_nodes[0]['name'] is not None\n",
    "            print(f\"   ✅ Name field present: {has_name}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Validation error: {e}\")\n",
    "\n",
    "# MAIN EXECUTION\n",
    "def process_updated_df_complete(updated_df):\n",
    "    \"\"\"\n",
    "    Complete processing function with robust error handling.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting KG JSON processing with UUID conversion...\")\n",
    "    print(\"🛡️  Enhanced error handling for missing names and malformed JSON\")\n",
    "    \n",
    "    # Show sample of original data\n",
    "    print(f\"\\n📝 Sample original kg_json (first 150 characters):\")\n",
    "    print(updated_df['kg_json'].iloc[0][:150] + \"...\")\n",
    "    \n",
    "    # Process the dataframe\n",
    "    processed_df, error_stats = quick_process_updated_df(updated_df)\n",
    "    \n",
    "    # Show sample of processed data\n",
    "    print(f\"\\n✅ Sample processed kg_json (first 150 characters):\")\n",
    "    print(processed_df['kg_json'].iloc[0][:150] + \"...\")\n",
    "    \n",
    "    # Validate the conversion\n",
    "    validate_conversion(updated_df, processed_df)\n",
    "    \n",
    "    # Show final statistics\n",
    "    print(f\"\\n📊 Final Statistics:\")\n",
    "    print(f\"   Original rows: {len(updated_df)}\")\n",
    "    print(f\"   Processed rows: {len(processed_df)}\")\n",
    "    print(f\"   Successful conversions: {len(updated_df) - error_stats['total_errors']}\")\n",
    "    print(f\"   Rows with errors (kept original): {error_stats['total_errors']}\")\n",
    "    print(f\"   Overall success rate: {((len(updated_df) - error_stats['total_errors']) / len(updated_df) * 100):.1f}%\")\n",
    "    \n",
    "    print(f\"\\n🎉 Processing complete! Use the returned dataframe.\")\n",
    "    print(f\"💡 Rows with errors kept their original JSON format\")\n",
    "    \n",
    "    return processed_df, error_stats\n",
    "\n",
    "# Call this function with your dataframe:\n",
    "# processed_df, error_stats = process_updated_df_complete(updated_df)\n",
    "\n",
    "print(\"✅ Enhanced functions loaded! Now handles missing names and JSON errors.\")\n",
    "print(\"📞 Call: processed_df, error_stats = process_updated_df_complete(updated_df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting processing of 2187 rows...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting KG JSON processing with UUID conversion...\n",
      "🛡️  Enhanced error handling for missing names and malformed JSON\n",
      "\n",
      "📝 Sample original kg_json (first 150 characters):\n",
      "[\n",
      "  {\n",
      "    \"metadata\": {\n",
      "      \"generated_at\": \"2024-11-02T10:00:00Z\",\n",
      "      \"total_nodes\": 3,\n",
      "      \"total_relationships\": 1,\n",
      "      \"entity_types\": [\n",
      "...\n",
      "📊 Processing dataframe with 2187 rows...\n",
      "📋 Columns: ['chapter', 'chunk', 'chunk_order_number', 'author', 'book', 'kg_json']\n",
      "Processed 200/2187 rows... (Errors so far: 0)\n",
      "Processed 400/2187 rows... (Errors so far: 0)\n",
      "Processed 600/2187 rows... (Errors so far: 0)\n",
      "Processed 800/2187 rows... (Errors so far: 0)\n",
      "Processed 1000/2187 rows... (Errors so far: 0)\n",
      "Processed 1200/2187 rows... (Errors so far: 0)\n",
      "Processed 1400/2187 rows... (Errors so far: 0)\n",
      "Processed 1600/2187 rows... (Errors so far: 0)\n",
      "Processed 1800/2187 rows... (Errors so far: 0)\n",
      "Processed 2000/2187 rows... (Errors so far: 0)\n",
      "Processing complete! Processed 2187 rows with 0 errors.\n",
      "⏱️  Processing completed in 0.30 seconds\n",
      "📈 Average time per row: 0.14 ms\n",
      "\n",
      "📊 Error Statistics:\n",
      "   Total errors: 0\n",
      "   JSON decode errors: 0\n",
      "   Missing name errors: 0\n",
      "   Other errors: 0\n",
      "   Success rate: 100.0%\n",
      "\n",
      "✅ Sample processed kg_json (first 150 characters):\n",
      "[{\"metadata\": {\"generated_at\": \"2024-11-02T10:00:00Z\", \"total_nodes\": 3, \"total_relationships\": 1, \"entity_types\": [\"Book\", \"Author\", \"Chunk\", \"Chapte...\n",
      "\n",
      "🔍 Validating conversion...\n",
      "   Original nodes: 3\n",
      "   Converted nodes: 3\n",
      "   Sample original ID: 1\n",
      "   Sample converted ID: f32bd123-42c0-5ba0-a943-797dc52d7e1a\n",
      "   ✅ Valid UUID format\n",
      "   ✅ Name field present: True\n",
      "\n",
      "📊 Final Statistics:\n",
      "   Original rows: 2187\n",
      "   Processed rows: 2187\n",
      "   Successful conversions: 2187\n",
      "   Rows with errors (kept original): 0\n",
      "   Overall success rate: 100.0%\n",
      "\n",
      "🎉 Processing complete! Use the returned dataframe.\n",
      "💡 Rows with errors kept their original JSON format\n"
     ]
    }
   ],
   "source": [
    "processed_df, error_stats = process_updated_df_complete(updated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'metadata': {'generated_at': '2025-06-01T16:39:21Z',\n",
       "   'total_nodes': 6,\n",
       "   'total_relationships': 7,\n",
       "   'entity_types': ['Actor',\n",
       "    'Book',\n",
       "    'Author',\n",
       "    'Chapter',\n",
       "    'Chunk',\n",
       "    'Intangible']},\n",
       "  'nodes': [{'id': 'f1912760-8749-51e0-908a-70e6ce194ca4',\n",
       "    'label': 'Author',\n",
       "    'name': 'Mark Twain',\n",
       "    'properties': {'nationality': 'American'},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'id': 'f32bd123-42c0-5ba0-a943-797dc52d7e1a',\n",
       "    'label': 'Book',\n",
       "    'name': 'ADVENTURES OF HUCKLEBERRY FINN',\n",
       "    'properties': {'genre': 'Adventure Fiction'},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'id': 'a3df4c5c-9289-59f5-91c2-d52144590f46',\n",
       "    'label': 'Chapter',\n",
       "    'name': 'CHAPTER XXIX.',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'id': '74e4287e-f952-5693-8bd0-db23bc6e4832',\n",
       "    'label': 'Chunk',\n",
       "    'text': '“He _can’t_ write with his left hand,” says the old gentleman. “If he\\ncould use his right hand, you would see that he wrote his own letters\\nand mine too. Look at both, please—they’re by the same hand.”',\n",
       "    'properties': {'chunk_order_number': 1412},\n",
       "    'timestamp': '2025-06-01T16:39:21Z',\n",
       "    'name': 'Chunk_4'},\n",
       "   {'id': 'cfb689ef-f3ee-53ab-bbaa-f2df46eeb3a8',\n",
       "    'label': 'Actor',\n",
       "    'name': 'old gentleman',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'id': 'ffcd94b1-3120-5189-92eb-19a616528b90',\n",
       "    'label': 'Intangible',\n",
       "    'name': 'writing ability',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'}],\n",
       "  'relationships': [{'start_id': 'f32bd123-42c0-5ba0-a943-797dc52d7e1a',\n",
       "    'end_id': 'f1912760-8749-51e0-908a-70e6ce194ca4',\n",
       "    'relationship_type': 'WRITTEN_BY',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'start_id': 'a3df4c5c-9289-59f5-91c2-d52144590f46',\n",
       "    'end_id': 'f32bd123-42c0-5ba0-a943-797dc52d7e1a',\n",
       "    'relationship_type': 'PART_OF',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'start_id': '74e4287e-f952-5693-8bd0-db23bc6e4832',\n",
       "    'end_id': 'a3df4c5c-9289-59f5-91c2-d52144590f46',\n",
       "    'relationship_type': 'PART_OF',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'start_id': 'cfb689ef-f3ee-53ab-bbaa-f2df46eeb3a8',\n",
       "    'end_id': '74e4287e-f952-5693-8bd0-db23bc6e4832',\n",
       "    'relationship_type': 'CONTAINS_DIALOGUE_BY',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'start_id': '74e4287e-f952-5693-8bd0-db23bc6e4832',\n",
       "    'end_id': 'cfb689ef-f3ee-53ab-bbaa-f2df46eeb3a8',\n",
       "    'relationship_type': 'MENTIONS',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'start_id': '74e4287e-f952-5693-8bd0-db23bc6e4832',\n",
       "    'end_id': 'ffcd94b1-3120-5189-92eb-19a616528b90',\n",
       "    'relationship_type': 'CONVEYS',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'},\n",
       "   {'start_id': 'cfb689ef-f3ee-53ab-bbaa-f2df46eeb3a8',\n",
       "    'end_id': 'ffcd94b1-3120-5189-92eb-19a616528b90',\n",
       "    'relationship_type': 'INFLUENCED_BY',\n",
       "    'properties': {},\n",
       "    'timestamp': '2025-06-01T16:39:21Z'}]}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "json.loads(processed_df[\"kg_json\"].iloc[1411])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2187 entries, 0 to 2186\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   chapter             2187 non-null   object\n",
      " 1   chunk               2187 non-null   object\n",
      " 2   chunk_order_number  2187 non-null   int64 \n",
      " 3   author              2187 non-null   object\n",
      " 4   book                2187 non-null   object\n",
      " 5   kg_json             2187 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 102.6+ KB\n"
     ]
    }
   ],
   "source": [
    "processed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Entity extraction functions loaded!\n",
      "\n",
      "📞 Usage options:\n",
      "1. Combined: entities_df = extract_entities_combined(processed_df)\n",
      "2. Separate: nodes_df, relationships_df, unique_entities_df = extract_entities_separate(processed_df)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def extract_nodes_from_kg_json(processed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract all nodes from kg_json into individual DataFrame rows.\n",
    "    Retains original columns and adds node-specific columns.\n",
    "    \"\"\"\n",
    "    print(\"📊 Extracting nodes from kg_json...\")\n",
    "\n",
    "    node_rows = []\n",
    "    total_nodes = 0\n",
    "    errors = 0\n",
    "\n",
    "    for idx, row in processed_df.iterrows():\n",
    "        try:\n",
    "            # Parse the kg_json\n",
    "            kg_data = json.loads(row[\"kg_json\"])\n",
    "\n",
    "            # Handle both single chunk and list of chunks\n",
    "            chunks = [kg_data] if isinstance(kg_data, dict) else kg_data\n",
    "\n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                nodes = chunk.get(\"nodes\", [])\n",
    "\n",
    "                for node in nodes:\n",
    "                    # Create new row with original columns plus node data\n",
    "                    node_row = {\n",
    "                        # Original columns\n",
    "                        \"chapter\": row[\"chapter\"],\n",
    "                        \"chunk\": row[\"chunk\"],\n",
    "                        \"chunk_order_number\": row[\"chunk_order_number\"],\n",
    "                        \"author\": row[\"author\"],\n",
    "                        \"book\": row[\"book\"],\n",
    "                        # Node-specific columns\n",
    "                        \"entity_type\": \"node\",\n",
    "                        \"chunk_index\": chunk_idx,\n",
    "                        \"node_id\": node.get(\"id\", \"\"),\n",
    "                        \"node_label\": node.get(\"label\", \"\"),\n",
    "                        \"node_name\": node.get(\"name\", \"\"),\n",
    "                        \"node_description\": node.get(\"description\", \"\"),\n",
    "                        \"node_properties\": json.dumps(node.get(\"properties\", {})),\n",
    "                        \"node_timestamp\": node.get(\"timestamp\", \"\"),\n",
    "                        # For compatibility - these will be empty for nodes\n",
    "                        \"relationship_type\": \"\",\n",
    "                        \"start_node_id\": \"\",\n",
    "                        \"end_node_id\": \"\",\n",
    "                        \"relationship_weight\": None,\n",
    "                        \"relationship_properties\": \"{}\",\n",
    "                        \"relationship_timestamp\": \"\",\n",
    "                    }\n",
    "\n",
    "                    node_rows.append(node_row)\n",
    "                    total_nodes += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Row {idx}: Error extracting nodes - {e}\")\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ Extracted {total_nodes} nodes with {errors} errors\")\n",
    "    return pd.DataFrame(node_rows)\n",
    "\n",
    "\n",
    "def extract_relationships_from_kg_json(processed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract all relationships from kg_json into individual DataFrame rows.\n",
    "    Retains original columns and adds relationship-specific columns.\n",
    "    \"\"\"\n",
    "    print(\"🔗 Extracting relationships from kg_json...\")\n",
    "\n",
    "    relationship_rows = []\n",
    "    total_relationships = 0\n",
    "    errors = 0\n",
    "\n",
    "    for idx, row in processed_df.iterrows():\n",
    "        try:\n",
    "            # Parse the kg_json\n",
    "            kg_data = json.loads(row[\"kg_json\"])\n",
    "\n",
    "            # Handle both single chunk and list of chunks\n",
    "            chunks = [kg_data] if isinstance(kg_data, dict) else kg_data\n",
    "\n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                relationships = chunk.get(\"relationships\", [])\n",
    "\n",
    "                for rel in relationships:\n",
    "                    # Create new row with original columns plus relationship data\n",
    "                    rel_row = {\n",
    "                        # Original columns\n",
    "                        \"chapter\": row[\"chapter\"],\n",
    "                        \"chunk\": row[\"chunk\"],\n",
    "                        \"chunk_order_number\": row[\"chunk_order_number\"],\n",
    "                        \"author\": row[\"author\"],\n",
    "                        \"book\": row[\"book\"],\n",
    "                        # Relationship-specific columns\n",
    "                        \"entity_type\": \"relationship\",\n",
    "                        \"chunk_index\": chunk_idx,\n",
    "                        \"relationship_type\": rel.get(\"relationship_type\", \"\"),\n",
    "                        \"start_node_id\": rel.get(\"start_id\", \"\"),\n",
    "                        \"end_node_id\": rel.get(\"end_id\", \"\"),\n",
    "                        \"relationship_weight\": rel.get(\"weight\"),\n",
    "                        \"relationship_properties\": json.dumps(\n",
    "                            rel.get(\"properties\", {})\n",
    "                        ),\n",
    "                        \"relationship_timestamp\": rel.get(\"timestamp\", \"\"),\n",
    "                        # For compatibility - these will be empty for relationships\n",
    "                        \"node_id\": \"\",\n",
    "                        \"node_label\": \"\",\n",
    "                        \"node_name\": \"\",\n",
    "                        \"node_description\": \"\",\n",
    "                        \"node_properties\": \"{}\",\n",
    "                        \"node_timestamp\": \"\",\n",
    "                    }\n",
    "\n",
    "                    relationship_rows.append(rel_row)\n",
    "                    total_relationships += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Row {idx}: Error extracting relationships - {e}\")\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ Extracted {total_relationships} relationships with {errors} errors\")\n",
    "    return pd.DataFrame(relationship_rows)\n",
    "\n",
    "\n",
    "def extract_all_entities(processed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract both nodes and relationships into a single DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"🎯 Extracting all entities (nodes + relationships) from kg_json...\")\n",
    "\n",
    "    # Extract nodes and relationships separately\n",
    "    nodes_df = extract_nodes_from_kg_json(processed_df)\n",
    "    relationships_df = extract_relationships_from_kg_json(processed_df)\n",
    "\n",
    "    # Combine them\n",
    "    combined_df = pd.concat([nodes_df, relationships_df], ignore_index=True)\n",
    "\n",
    "    print(f\"🎉 Combined extraction complete!\")\n",
    "    print(f\"   Total nodes: {len(nodes_df)}\")\n",
    "    print(f\"   Total relationships: {len(relationships_df)}\")\n",
    "    print(f\"   Total entities: {len(combined_df)}\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def create_separate_dataframes(\n",
    "    processed_df: pd.DataFrame,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create separate DataFrames for nodes and relationships.\n",
    "    Returns: (nodes_df, relationships_df)\n",
    "    \"\"\"\n",
    "    print(\"📋 Creating separate DataFrames for nodes and relationships...\")\n",
    "\n",
    "    nodes_df = extract_nodes_from_kg_json(processed_df)\n",
    "    relationships_df = extract_relationships_from_kg_json(processed_df)\n",
    "\n",
    "    # Clean up columns - remove empty relationship columns from nodes_df\n",
    "    nodes_columns = [\n",
    "        \"chapter\",\n",
    "        \"chunk\",\n",
    "        \"chunk_order_number\",\n",
    "        \"author\",\n",
    "        \"book\",\n",
    "        \"entity_type\",\n",
    "        \"chunk_index\",\n",
    "        \"node_id\",\n",
    "        \"node_label\",\n",
    "        \"node_name\",\n",
    "        \"node_description\",\n",
    "        \"node_properties\",\n",
    "        \"node_timestamp\",\n",
    "    ]\n",
    "\n",
    "    # Clean up columns - remove empty node columns from relationships_df\n",
    "    relationships_columns = [\n",
    "        \"chapter\",\n",
    "        \"chunk\",\n",
    "        \"chunk_order_number\",\n",
    "        \"author\",\n",
    "        \"book\",\n",
    "        \"entity_type\",\n",
    "        \"chunk_index\",\n",
    "        \"relationship_type\",\n",
    "        \"start_node_id\",\n",
    "        \"end_node_id\",\n",
    "        \"relationship_weight\",\n",
    "        \"relationship_properties\",\n",
    "        \"relationship_timestamp\",\n",
    "    ]\n",
    "\n",
    "    clean_nodes_df = nodes_df[nodes_columns].copy()\n",
    "    clean_relationships_df = relationships_df[relationships_columns].copy()\n",
    "\n",
    "    return clean_nodes_df, clean_relationships_df\n",
    "\n",
    "\n",
    "def analyze_extraction_results(\n",
    "    nodes_df: pd.DataFrame, relationships_df: pd.DataFrame, original_df: pd.DataFrame\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze and display statistics about the extraction.\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Extraction Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"Original DataFrame:\")\n",
    "    print(f\"  Rows: {len(original_df)}\")\n",
    "    print(f\"  Columns: {list(original_df.columns)}\")\n",
    "\n",
    "    print(f\"\\nNodes DataFrame:\")\n",
    "    print(f\"  Rows: {len(nodes_df)}\")\n",
    "    print(f\"  Columns: {list(nodes_df.columns)}\")\n",
    "\n",
    "    print(f\"\\nRelationships DataFrame:\")\n",
    "    print(f\"  Rows: {len(relationships_df)}\")\n",
    "    print(f\"  Columns: {list(relationships_df.columns)}\")\n",
    "\n",
    "    # Analyze node types\n",
    "    if len(nodes_df) > 0:\n",
    "        print(f\"\\nNode Label Distribution:\")\n",
    "        label_counts = nodes_df[\"node_label\"].value_counts()\n",
    "        for label, count in label_counts.head(10).items():\n",
    "            print(f\"  {label}: {count}\")\n",
    "\n",
    "    # Analyze relationship types\n",
    "    if len(relationships_df) > 0:\n",
    "        print(f\"\\nRelationship Type Distribution:\")\n",
    "        rel_counts = relationships_df[\"relationship_type\"].value_counts()\n",
    "        for rel_type, count in rel_counts.head(10).items():\n",
    "            print(f\"  {rel_type}: {count}\")\n",
    "\n",
    "    # Show sample data\n",
    "    print(f\"\\nSample Node Row:\")\n",
    "    if len(nodes_df) > 0:\n",
    "        sample_node = nodes_df.iloc[0]\n",
    "        for col in [\"chapter\", \"node_label\", \"node_name\", \"node_id\"]:\n",
    "            print(f\"  {col}: {sample_node[col]}\")\n",
    "\n",
    "    print(f\"\\nSample Relationship Row:\")\n",
    "    if len(relationships_df) > 0:\n",
    "        sample_rel = relationships_df.iloc[0]\n",
    "        for col in [\"chapter\", \"relationship_type\", \"start_node_id\", \"end_node_id\"]:\n",
    "            print(f\"  {col}: {sample_rel[col]}\")\n",
    "\n",
    "\n",
    "def get_unique_entities_summary(nodes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get summary of unique entities across the entire knowledge graph.\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 Analyzing unique entities...\")\n",
    "\n",
    "    # Group by node_id to get unique entities\n",
    "    unique_entities = (\n",
    "        nodes_df.groupby(\"node_id\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"node_label\": \"first\",\n",
    "                \"node_name\": \"first\",\n",
    "                \"node_description\": \"first\",\n",
    "                \"chapter\": lambda x: list(\n",
    "                    set(x)\n",
    "                ),  # Unique chapters where this entity appears\n",
    "                \"book\": \"first\",\n",
    "                \"author\": \"first\",\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Add appearance count\n",
    "    appearance_counts = nodes_df[\"node_id\"].value_counts()\n",
    "    unique_entities[\"appearance_count\"] = unique_entities[\"node_id\"].map(\n",
    "        appearance_counts\n",
    "    )\n",
    "\n",
    "    # Sort by appearance count\n",
    "    unique_entities = unique_entities.sort_values(\"appearance_count\", ascending=False)\n",
    "\n",
    "    print(f\"✅ Found {len(unique_entities)} unique entities\")\n",
    "    print(f\"📈 Top entities by appearance:\")\n",
    "\n",
    "    top_entities = unique_entities.head(10)\n",
    "    for _, entity in top_entities.iterrows():\n",
    "        chapters = (\n",
    "            entity[\"chapter\"]\n",
    "            if isinstance(entity[\"chapter\"], list)\n",
    "            else [entity[\"chapter\"]]\n",
    "        )\n",
    "        chapter_str = (\n",
    "            f\"({len(chapters)} chapters)\"\n",
    "            if len(chapters) > 1\n",
    "            else f\"Chapter: {chapters[0]}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  {entity['node_label']}: {entity['node_name']} - {entity['appearance_count']} times {chapter_str}\"\n",
    "        )\n",
    "\n",
    "    return unique_entities\n",
    "\n",
    "\n",
    "# MAIN EXECUTION FUNCTIONS\n",
    "def extract_entities_combined(processed_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function: Extract all entities into a single DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting entity extraction (combined)...\")\n",
    "\n",
    "    combined_df = extract_all_entities(processed_df)\n",
    "\n",
    "    print(f\"\\n📊 Extraction Summary:\")\n",
    "    print(f\"   Input rows: {len(processed_df)}\")\n",
    "    print(f\"   Output entity rows: {len(combined_df)}\")\n",
    "    print(f\"   Expansion ratio: {len(combined_df)/len(processed_df):.1f}x\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def extract_entities_separate(\n",
    "    processed_df: pd.DataFrame,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Main function: Extract entities into separate DataFrames.\n",
    "    Returns: (nodes_df, relationships_df, unique_entities_df)\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting entity extraction (separate DataFrames)...\")\n",
    "\n",
    "    nodes_df, relationships_df = create_separate_dataframes(processed_df)\n",
    "    unique_entities_df = get_unique_entities_summary(nodes_df)\n",
    "\n",
    "    analyze_extraction_results(nodes_df, relationships_df, processed_df)\n",
    "\n",
    "    return nodes_df, relationships_df, unique_entities_df\n",
    "\n",
    "\n",
    "# Usage examples:\n",
    "print(\"✅ Entity extraction functions loaded!\")\n",
    "print(\"\\n📞 Usage options:\")\n",
    "print(\"1. Combined: entities_df = extract_entities_combined(processed_df)\")\n",
    "print(\n",
    "    \"2. Separate: nodes_df, relationships_df, unique_entities_df = extract_entities_separate(processed_df)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting entity extraction (combined)...\n",
      "🎯 Extracting all entities (nodes + relationships) from kg_json...\n",
      "📊 Extracting nodes from kg_json...\n",
      "✅ Extracted 16090 nodes with 0 errors\n",
      "🔗 Extracting relationships from kg_json...\n",
      "✅ Extracted 18014 relationships with 0 errors\n",
      "🎉 Combined extraction complete!\n",
      "   Total nodes: 16090\n",
      "   Total relationships: 18014\n",
      "   Total entities: 34104\n",
      "\n",
      "📊 Extraction Summary:\n",
      "   Input rows: 2187\n",
      "   Output entity rows: 34104\n",
      "   Expansion ratio: 15.6x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/2xg7pthj21x9b1rhc0tl_bnh0000gn/T/ipykernel_80264/594494597.py:146: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([nodes_df, relationships_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# After running your UUID processing:\n",
    "# processed_df, error_stats = process_updated_df_complete(updated_df)\n",
    "\n",
    "# Extract entities (choose one approach):\n",
    "\n",
    "# Option 1: Everything together\n",
    "entities_df = extract_entities_combined(processed_df)\n",
    "\n",
    "# Option 2: Separate analysis\n",
    "# nodes_df, relationships_df, unique_entities_df = extract_entities_separate(processed_df)\n",
    "\n",
    "# Now you have individual rows for each entity with full metadata!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_order_number</th>\n",
       "      <th>author</th>\n",
       "      <th>book</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>node_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>node_name</th>\n",
       "      <th>node_description</th>\n",
       "      <th>node_properties</th>\n",
       "      <th>node_timestamp</th>\n",
       "      <th>relationship_type</th>\n",
       "      <th>start_node_id</th>\n",
       "      <th>end_node_id</th>\n",
       "      <th>relationship_weight</th>\n",
       "      <th>relationship_properties</th>\n",
       "      <th>relationship_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>ADVENTURES\\nOF\\nHUCKLEBERRY FINN</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>node</td>\n",
       "      <td>0</td>\n",
       "      <td>f32bd123-42c0-5ba0-a943-797dc52d7e1a</td>\n",
       "      <td>Book</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>Classic American novel by Mark Twain</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-06-01T16:36:15Z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>ADVENTURES\\nOF\\nHUCKLEBERRY FINN</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>node</td>\n",
       "      <td>0</td>\n",
       "      <td>f1912760-8749-51e0-908a-70e6ce194ca4</td>\n",
       "      <td>Author</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>American author</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-06-01T16:36:15Z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>ADVENTURES\\nOF\\nHUCKLEBERRY FINN</td>\n",
       "      <td>1</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>node</td>\n",
       "      <td>0</td>\n",
       "      <td>1bcd403f-f184-58af-a991-abee627ceca8</td>\n",
       "      <td>Chapter</td>\n",
       "      <td>Preamble</td>\n",
       "      <td>The Preamble of the book.</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-06-01T16:36:15Z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>(Tom Sawyer’s Comrade)</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>node</td>\n",
       "      <td>0</td>\n",
       "      <td>f32bd123-42c0-5ba0-a943-797dc52d7e1a</td>\n",
       "      <td>Book</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>Classic American novel</td>\n",
       "      <td>{\"author\": \"Mark Twain\"}</td>\n",
       "      <td>2025-06-01T16:36:15Z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preamble</td>\n",
       "      <td>(Tom Sawyer’s Comrade)</td>\n",
       "      <td>2</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>node</td>\n",
       "      <td>0</td>\n",
       "      <td>f1912760-8749-51e0-908a-70e6ce194ca4</td>\n",
       "      <td>Author</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>Author of ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>{}</td>\n",
       "      <td>2025-06-01T16:36:15Z</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34099</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>Tom’s most well now, and got his bullet around...</td>\n",
       "      <td>2186</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>CONVEYS</td>\n",
       "      <td>c4242b73-9976-53ae-8fb8-da9b6de337f2</td>\n",
       "      <td>99c5329c-d511-53f1-af10-bb903c7649bb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34100</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>Tom’s most well now, and got his bullet around...</td>\n",
       "      <td>2186</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>CONTAINS_DIALOGUE_BY</td>\n",
       "      <td>c4242b73-9976-53ae-8fb8-da9b6de337f2</td>\n",
       "      <td>51a17aa9-af7b-5ec4-896c-4ee7ce8eb1c0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34101</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>Tom’s most well now, and got his bullet around...</td>\n",
       "      <td>2186</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>CONTAINS</td>\n",
       "      <td>f32bd123-42c0-5ba0-a943-797dc52d7e1a</td>\n",
       "      <td>b518aab7-c3e5-5a46-864b-ba4178ccd28b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34102</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>THE END. YOURS TRULY, _HUCK FINN_.</td>\n",
       "      <td>2187</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>WRITTEN_BY</td>\n",
       "      <td>f32bd123-42c0-5ba0-a943-797dc52d7e1a</td>\n",
       "      <td>f1912760-8749-51e0-908a-70e6ce194ca4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{}</td>\n",
       "      <td>2024-11-02T16:40:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34103</th>\n",
       "      <td>CHAPTER XLII.</td>\n",
       "      <td>THE END. YOURS TRULY, _HUCK FINN_.</td>\n",
       "      <td>2187</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>ADVENTURES OF HUCKLEBERRY FINN</td>\n",
       "      <td>relationship</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>PART_OF</td>\n",
       "      <td>b518aab7-c3e5-5a46-864b-ba4178ccd28b</td>\n",
       "      <td>f32bd123-42c0-5ba0-a943-797dc52d7e1a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{}</td>\n",
       "      <td>2024-11-02T16:40:58Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34104 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             chapter                                              chunk  \\\n",
       "0           Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
       "1           Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
       "2           Preamble                   ADVENTURES\\nOF\\nHUCKLEBERRY FINN   \n",
       "3           Preamble                             (Tom Sawyer’s Comrade)   \n",
       "4           Preamble                             (Tom Sawyer’s Comrade)   \n",
       "...              ...                                                ...   \n",
       "34099  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
       "34100  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
       "34101  CHAPTER XLII.  Tom’s most well now, and got his bullet around...   \n",
       "34102  CHAPTER XLII.                 THE END. YOURS TRULY, _HUCK FINN_.   \n",
       "34103  CHAPTER XLII.                 THE END. YOURS TRULY, _HUCK FINN_.   \n",
       "\n",
       "       chunk_order_number      author                            book  \\\n",
       "0                       1  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "1                       1  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "2                       1  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "3                       2  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "4                       2  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "...                   ...         ...                             ...   \n",
       "34099                2186  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "34100                2186  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "34101                2186  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "34102                2187  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "34103                2187  Mark Twain  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "\n",
       "        entity_type  chunk_index                               node_id  \\\n",
       "0              node            0  f32bd123-42c0-5ba0-a943-797dc52d7e1a   \n",
       "1              node            0  f1912760-8749-51e0-908a-70e6ce194ca4   \n",
       "2              node            0  1bcd403f-f184-58af-a991-abee627ceca8   \n",
       "3              node            0  f32bd123-42c0-5ba0-a943-797dc52d7e1a   \n",
       "4              node            0  f1912760-8749-51e0-908a-70e6ce194ca4   \n",
       "...             ...          ...                                   ...   \n",
       "34099  relationship            0                                         \n",
       "34100  relationship            0                                         \n",
       "34101  relationship            0                                         \n",
       "34102  relationship            0                                         \n",
       "34103  relationship            0                                         \n",
       "\n",
       "      node_label                       node_name  \\\n",
       "0           Book  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "1         Author                      Mark Twain   \n",
       "2        Chapter                        Preamble   \n",
       "3           Book  ADVENTURES OF HUCKLEBERRY FINN   \n",
       "4         Author                      Mark Twain   \n",
       "...          ...                             ...   \n",
       "34099                                              \n",
       "34100                                              \n",
       "34101                                              \n",
       "34102                                              \n",
       "34103                                              \n",
       "\n",
       "                               node_description           node_properties  \\\n",
       "0          Classic American novel by Mark Twain                        {}   \n",
       "1                               American author                        {}   \n",
       "2                     The Preamble of the book.                        {}   \n",
       "3                        Classic American novel  {\"author\": \"Mark Twain\"}   \n",
       "4      Author of ADVENTURES OF HUCKLEBERRY FINN                        {}   \n",
       "...                                         ...                       ...   \n",
       "34099                                                                  {}   \n",
       "34100                                                                  {}   \n",
       "34101                                                                  {}   \n",
       "34102                                                                  {}   \n",
       "34103                                                                  {}   \n",
       "\n",
       "             node_timestamp     relationship_type  \\\n",
       "0      2025-06-01T16:36:15Z                         \n",
       "1      2025-06-01T16:36:15Z                         \n",
       "2      2025-06-01T16:36:15Z                         \n",
       "3      2025-06-01T16:36:15Z                         \n",
       "4      2025-06-01T16:36:15Z                         \n",
       "...                     ...                   ...   \n",
       "34099                                     CONVEYS   \n",
       "34100                        CONTAINS_DIALOGUE_BY   \n",
       "34101                                    CONTAINS   \n",
       "34102                                  WRITTEN_BY   \n",
       "34103                                     PART_OF   \n",
       "\n",
       "                              start_node_id  \\\n",
       "0                                             \n",
       "1                                             \n",
       "2                                             \n",
       "3                                             \n",
       "4                                             \n",
       "...                                     ...   \n",
       "34099  c4242b73-9976-53ae-8fb8-da9b6de337f2   \n",
       "34100  c4242b73-9976-53ae-8fb8-da9b6de337f2   \n",
       "34101  f32bd123-42c0-5ba0-a943-797dc52d7e1a   \n",
       "34102  f32bd123-42c0-5ba0-a943-797dc52d7e1a   \n",
       "34103  b518aab7-c3e5-5a46-864b-ba4178ccd28b   \n",
       "\n",
       "                                end_node_id  relationship_weight  \\\n",
       "0                                                            NaN   \n",
       "1                                                            NaN   \n",
       "2                                                            NaN   \n",
       "3                                                            NaN   \n",
       "4                                                            NaN   \n",
       "...                                     ...                  ...   \n",
       "34099  99c5329c-d511-53f1-af10-bb903c7649bb                  NaN   \n",
       "34100  51a17aa9-af7b-5ec4-896c-4ee7ce8eb1c0                  NaN   \n",
       "34101  b518aab7-c3e5-5a46-864b-ba4178ccd28b                  NaN   \n",
       "34102  f1912760-8749-51e0-908a-70e6ce194ca4                  1.0   \n",
       "34103  f32bd123-42c0-5ba0-a943-797dc52d7e1a                  1.0   \n",
       "\n",
       "      relationship_properties relationship_timestamp  \n",
       "0                          {}                         \n",
       "1                          {}                         \n",
       "2                          {}                         \n",
       "3                          {}                         \n",
       "4                          {}                         \n",
       "...                       ...                    ...  \n",
       "34099                      {}                         \n",
       "34100                      {}                         \n",
       "34101                      {}                         \n",
       "34102                      {}   2024-11-02T16:40:58Z  \n",
       "34103                      {}   2024-11-02T16:40:58Z  \n",
       "\n",
       "[34104 rows x 19 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting entity extraction (separate DataFrames)...\n",
      "📋 Creating separate DataFrames for nodes and relationships...\n",
      "📊 Extracting nodes from kg_json...\n",
      "✅ Extracted 16090 nodes with 0 errors\n",
      "🔗 Extracting relationships from kg_json...\n",
      "✅ Extracted 18014 relationships with 0 errors\n",
      "\n",
      "🔍 Analyzing unique entities...\n",
      "✅ Found 5305 unique entities\n",
      "📈 Top entities by appearance:\n",
      "  Book: ADVENTURES OF HUCKLEBERRY FINN - 2185 times (43 chapters)\n",
      "  Author: Mark Twain - 2131 times (43 chapters)\n",
      "  Actor: I - 327 times (40 chapters)\n",
      "  Actor: Jim - 271 times (32 chapters)\n",
      "  Actor: Tom - 122 times (17 chapters)\n",
      "  Chapter: CHAPTER XXVI. - 100 times Chapter: CHAPTER XXVI.\n",
      "  Actor: Narrator - 95 times (36 chapters)\n",
      "  Chapter: CHAPTER XXVIII. - 95 times Chapter: CHAPTER XXVIII.\n",
      "  Chapter: CHAPTER XXIX. - 89 times Chapter: CHAPTER XXIX.\n",
      "  Chapter: CHAPTER XLII. - 86 times Chapter: CHAPTER XLII.\n",
      "\n",
      "📊 Extraction Analysis:\n",
      "==================================================\n",
      "Original DataFrame:\n",
      "  Rows: 2187\n",
      "  Columns: ['chapter', 'chunk', 'chunk_order_number', 'author', 'book', 'kg_json']\n",
      "\n",
      "Nodes DataFrame:\n",
      "  Rows: 16090\n",
      "  Columns: ['chapter', 'chunk', 'chunk_order_number', 'author', 'book', 'entity_type', 'chunk_index', 'node_id', 'node_label', 'node_name', 'node_description', 'node_properties', 'node_timestamp']\n",
      "\n",
      "Relationships DataFrame:\n",
      "  Rows: 18014\n",
      "  Columns: ['chapter', 'chunk', 'chunk_order_number', 'author', 'book', 'entity_type', 'chunk_index', 'relationship_type', 'start_node_id', 'end_node_id', 'relationship_weight', 'relationship_properties', 'relationship_timestamp']\n",
      "\n",
      "Node Label Distribution:\n",
      "  Actor: 3641\n",
      "  Chunk: 2264\n",
      "  Book: 2186\n",
      "  Chapter: 2186\n",
      "  Author: 2132\n",
      "  Object: 1606\n",
      "  Intangible: 938\n",
      "  Location: 639\n",
      "  Event: 498\n",
      "\n",
      "Relationship Type Distribution:\n",
      "  CONTAINS: 2622\n",
      "  MENTIONS: 2407\n",
      "  PART_OF: 2064\n",
      "  CONTAINS_DIALOGUE_BY: 1486\n",
      "  WRITTEN_BY: 1220\n",
      "  WROTE: 1204\n",
      "  REFERENCES: 994\n",
      "  INTERACTED_WITH: 660\n",
      "  USED: 602\n",
      "  CONVEYS: 542\n",
      "\n",
      "Sample Node Row:\n",
      "  chapter: Preamble\n",
      "  node_label: Book\n",
      "  node_name: ADVENTURES OF HUCKLEBERRY FINN\n",
      "  node_id: f32bd123-42c0-5ba0-a943-797dc52d7e1a\n",
      "\n",
      "Sample Relationship Row:\n",
      "  chapter: Preamble\n",
      "  relationship_type: WRITTEN_BY\n",
      "  start_node_id: f32bd123-42c0-5ba0-a943-797dc52d7e1a\n",
      "  end_node_id: f1912760-8749-51e0-908a-70e6ce194ca4\n"
     ]
    }
   ],
   "source": [
    "# After running your UUID processing:\n",
    "# processed_df, error_stats = process_updated_df_complete(updated_df)\n",
    "\n",
    "# Extract entities (choose one approach):\n",
    "\n",
    "# Option 1: Everything together\n",
    "# entities_df = extract_entities_combined(processed_df)\n",
    "\n",
    "# Option 2: Separate analysis\n",
    "nodes_df, relationships_df, unique_entities_df = extract_entities_separate(processed_df)\n",
    "\n",
    "# Now you have individual rows for each entity with full metadata!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Complete Memgraph conversion functions loaded!\n",
      "📞 Call: memgraph_files = convert_to_memgraph_complete(entities_df)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import re\n",
    "\n",
    "def clean_for_csv(value: str) -> str:\n",
    "    \"\"\"Clean string values for CSV export to avoid import issues.\"\"\"\n",
    "    if pd.isna(value) or value is None:\n",
    "        return \"\"\n",
    "    \n",
    "    value = str(value)\n",
    "    value = value.replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n",
    "    value = value.replace('\"', '\\\\\"')\n",
    "    value = value.replace('\\t', ' ')\n",
    "    \n",
    "    if len(value) > 1000:\n",
    "        value = value[:997] + \"...\"\n",
    "    \n",
    "    return value\n",
    "\n",
    "def format_properties_for_memgraph(properties_json: str, additional_props: Dict[str, Any] = None) -> str:\n",
    "    \"\"\"Format properties JSON for Memgraph import.\"\"\"\n",
    "    try:\n",
    "        if properties_json and properties_json != '{}':\n",
    "            props = json.loads(properties_json)\n",
    "        else:\n",
    "            props = {}\n",
    "        \n",
    "        if additional_props:\n",
    "            props.update(additional_props)\n",
    "        \n",
    "        cleaned_props = {}\n",
    "        for key, value in props.items():\n",
    "            if value is not None and value != \"\":\n",
    "                clean_key = re.sub(r'[^a-zA-Z0-9_]', '_', str(key))\n",
    "                \n",
    "                if isinstance(value, (int, float)):\n",
    "                    cleaned_props[clean_key] = value\n",
    "                elif isinstance(value, bool):\n",
    "                    cleaned_props[clean_key] = value\n",
    "                else:\n",
    "                    cleaned_props[clean_key] = clean_for_csv(str(value))\n",
    "        \n",
    "        return json.dumps(cleaned_props) if cleaned_props else \"{}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error formatting properties: {e}\")\n",
    "        return \"{}\"\n",
    "\n",
    "def prepare_nodes_for_memgraph(entities_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare nodes data for Memgraph import.\"\"\"\n",
    "    print(\"📊 Preparing nodes for Memgraph import...\")\n",
    "    \n",
    "    nodes_df = entities_df[entities_df['entity_type'] == 'node'].copy()\n",
    "    \n",
    "    if len(nodes_df) == 0:\n",
    "        print(\"⚠️  No nodes found in entities_df\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    memgraph_nodes = pd.DataFrame()\n",
    "    memgraph_nodes['id'] = nodes_df['node_id']\n",
    "    memgraph_nodes['labels'] = nodes_df['node_label']\n",
    "    \n",
    "    properties_list = []\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        additional_props = {\n",
    "            'name': clean_for_csv(row['node_name']),\n",
    "            'description': clean_for_csv(row['node_description']),\n",
    "            'chapter': clean_for_csv(row['chapter']),\n",
    "            'chunk': clean_for_csv(row['chunk']),\n",
    "            'chunk_order_number': row['chunk_order_number'],\n",
    "            'author': clean_for_csv(row['author']),\n",
    "            'book': clean_for_csv(row['book']),\n",
    "            'chunk_index': row.get('chunk_index', 0),\n",
    "            'timestamp': clean_for_csv(row['node_timestamp'])\n",
    "        }\n",
    "        \n",
    "        formatted_props = format_properties_for_memgraph(\n",
    "            row['node_properties'], \n",
    "            additional_props\n",
    "        )\n",
    "        properties_list.append(formatted_props)\n",
    "    \n",
    "    memgraph_nodes['properties'] = properties_list\n",
    "    \n",
    "    initial_count = len(memgraph_nodes)\n",
    "    memgraph_nodes = memgraph_nodes.drop_duplicates(subset=['id'], keep='first')\n",
    "    final_count = len(memgraph_nodes)\n",
    "    \n",
    "    if initial_count > final_count:\n",
    "        print(f\"🔍 Removed {initial_count - final_count} duplicate nodes\")\n",
    "    \n",
    "    print(f\"✅ Prepared {len(memgraph_nodes)} unique nodes\")\n",
    "    return memgraph_nodes\n",
    "\n",
    "def prepare_relationships_for_memgraph(entities_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare relationships data for Memgraph import.\"\"\"\n",
    "    print(\"🔗 Preparing relationships for Memgraph import...\")\n",
    "    \n",
    "    rels_df = entities_df[entities_df['entity_type'] == 'relationship'].copy()\n",
    "    \n",
    "    if len(rels_df) == 0:\n",
    "        print(\"⚠️  No relationships found in entities_df\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    memgraph_rels = pd.DataFrame()\n",
    "    memgraph_rels['start_id'] = rels_df['start_node_id']\n",
    "    memgraph_rels['end_id'] = rels_df['end_node_id']\n",
    "    memgraph_rels['type'] = rels_df['relationship_type']\n",
    "    \n",
    "    properties_list = []\n",
    "    for _, row in rels_df.iterrows():\n",
    "        additional_props = {\n",
    "            'weight': row['relationship_weight'] if pd.notna(row['relationship_weight']) else 1.0,\n",
    "            'chapter': clean_for_csv(row['chapter']),\n",
    "            'chunk': clean_for_csv(row['chunk']),\n",
    "            'chunk_order_number': row['chunk_order_number'],\n",
    "            'author': clean_for_csv(row['author']),\n",
    "            'book': clean_for_csv(row['book']),\n",
    "            'chunk_index': row.get('chunk_index', 0),\n",
    "            'timestamp': clean_for_csv(row['relationship_timestamp'])\n",
    "        }\n",
    "        \n",
    "        formatted_props = format_properties_for_memgraph(\n",
    "            row['relationship_properties'], \n",
    "            additional_props\n",
    "        )\n",
    "        properties_list.append(formatted_props)\n",
    "    \n",
    "    memgraph_rels['properties'] = properties_list\n",
    "    \n",
    "    initial_count = len(memgraph_rels)\n",
    "    memgraph_rels = memgraph_rels.dropna(subset=['start_id', 'end_id'])\n",
    "    memgraph_rels = memgraph_rels[(memgraph_rels['start_id'] != '') & (memgraph_rels['end_id'] != '')]\n",
    "    final_count = len(memgraph_rels)\n",
    "    \n",
    "    if initial_count > final_count:\n",
    "        print(f\"🔍 Removed {initial_count - final_count} relationships with missing node references\")\n",
    "    \n",
    "    print(f\"✅ Prepared {len(memgraph_rels)} relationships\")\n",
    "    return memgraph_rels\n",
    "\n",
    "def export_to_csv(nodes_df: pd.DataFrame, relationships_df: pd.DataFrame, output_dir: str = \"memgraph_import\") -> Tuple[str, str]:\n",
    "    \"\"\"Export nodes and relationships to CSV files for Memgraph import.\"\"\"\n",
    "    print(f\"💾 Exporting to CSV files in '{output_dir}' directory...\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    nodes_file = os.path.join(output_dir, \"nodes.csv\")\n",
    "    relationships_file = os.path.join(output_dir, \"relationships.csv\")\n",
    "    \n",
    "    if len(nodes_df) > 0:\n",
    "        nodes_df.to_csv(nodes_file, index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "        print(f\"✅ Exported {len(nodes_df)} nodes to {nodes_file}\")\n",
    "    else:\n",
    "        print(\"⚠️  No nodes to export\")\n",
    "    \n",
    "    if len(relationships_df) > 0:\n",
    "        relationships_df.to_csv(relationships_file, index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "        print(f\"✅ Exported {len(relationships_df)} relationships to {relationships_file}\")\n",
    "    else:\n",
    "        print(\"⚠️  No relationships to export\")\n",
    "    \n",
    "    return nodes_file, relationships_file\n",
    "\n",
    "def generate_cypher_import_script(nodes_file: str, relationships_file: str, output_dir: str = \"memgraph_import\") -> str:\n",
    "    \"\"\"Generate Memgraph-specific Cypher script for importing data.\"\"\"\n",
    "    print(\"📝 Generating Memgraph Cypher import script...\")\n",
    "    \n",
    "    script_content = f\"\"\"// Memgraph Import Script\n",
    "// Generated from entities_df conversion\n",
    "// Compatible with Memgraph (no APOC dependencies)\n",
    "\n",
    "// Clear existing data (CAREFUL!)\n",
    "// MATCH (n) DETACH DELETE n;\n",
    "\n",
    "// Create index for faster lookups\n",
    "CREATE INDEX ON :Author(id);\n",
    "CREATE INDEX ON :Book(id);\n",
    "CREATE INDEX ON :Chapter(id);\n",
    "CREATE INDEX ON :Chunk(id);\n",
    "CREATE INDEX ON :Actor(id);\n",
    "CREATE INDEX ON :Object(id);\n",
    "\n",
    "// Import Nodes\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "CALL {{\n",
    "  WITH row\n",
    "  WITH row.id as nodeId, row.labels as nodeLabel, row.properties as propsJson\n",
    "  WITH nodeId, nodeLabel, \n",
    "       CASE \n",
    "         WHEN propsJson IS NULL OR propsJson = '' OR propsJson = '{{}}' \n",
    "         THEN {{}} \n",
    "         ELSE json.loads(propsJson) \n",
    "       END as props\n",
    "  // Create node with dynamic label\n",
    "  CALL {{ \n",
    "    WITH nodeId, nodeLabel, props\n",
    "    WITH \"CREATE (n:\" + nodeLabel + \" {{id: $nodeId}}) SET n += $props RETURN n\" as query\n",
    "    CALL query_module.call(query, {{nodeId: nodeId, props: props}}) YIELD result\n",
    "    RETURN result\n",
    "  }}\n",
    "}} IN TRANSACTIONS OF 1000 ROWS;\n",
    "\n",
    "// Import Relationships  \n",
    "LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "CALL {{\n",
    "  WITH row\n",
    "  WITH row.start_id as startId, row.end_id as endId, row.type as relType, row.properties as propsJson\n",
    "  WITH startId, endId, relType,\n",
    "       CASE \n",
    "         WHEN propsJson IS NULL OR propsJson = '' OR propsJson = '{{}}' \n",
    "         THEN {{}} \n",
    "         ELSE json.loads(propsJson) \n",
    "       END as props\n",
    "  MATCH (start {{id: startId}})\n",
    "  MATCH (end {{id: endId}})\n",
    "  // Create relationship with dynamic type\n",
    "  CALL {{\n",
    "    WITH start, end, relType, props\n",
    "    WITH \"CREATE (start)-[r:\" + relType + \"]->(end) SET r += $props RETURN r\" as query\n",
    "    CALL query_module.call(query, {{props: props}}) YIELD result\n",
    "    RETURN result\n",
    "  }}\n",
    "}} IN TRANSACTIONS OF 1000 ROWS;\n",
    "\n",
    "// Verification queries\n",
    "MATCH (n) RETURN labels(n) as label, count(n) as count ORDER BY count DESC;\n",
    "MATCH ()-[r]-() RETURN type(r) as relationship_type, count(r) as count ORDER BY count DESC;\n",
    "\"\"\"\n",
    "\n",
    "    # Alternative simpler script for manual execution\n",
    "    simple_script = f\"\"\"// Simple Memgraph Import Script\n",
    "// Execute these queries one by one in Memgraph Lab\n",
    "\n",
    "// Clear existing data (CAREFUL!)\n",
    "// MATCH (n) DETACH DELETE n;\n",
    "\n",
    "// Create indexes for performance\n",
    "CREATE INDEX ON :Author(id);\n",
    "CREATE INDEX ON :Book(id); \n",
    "CREATE INDEX ON :Chapter(id);\n",
    "CREATE INDEX ON :Chunk(id);\n",
    "CREATE INDEX ON :Actor(id);\n",
    "CREATE INDEX ON :Object(id);\n",
    "\n",
    "// Import Authors\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "WHERE row.labels = 'Author'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "CREATE (n:Author {{id: row.id}})\n",
    "SET n += props;\n",
    "\n",
    "// Import Books\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "WHERE row.labels = 'Book'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "CREATE (n:Book {{id: row.id}})\n",
    "SET n += props;\n",
    "\n",
    "// Import Chapters\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "WHERE row.labels = 'Chapter'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "CREATE (n:Chapter {{id: row.id}})\n",
    "SET n += props;\n",
    "\n",
    "// Import Chunks\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "WHERE row.labels = 'Chunk'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "CREATE (n:Chunk {{id: row.id}})\n",
    "SET n += props;\n",
    "\n",
    "// Import Actors\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "WHERE row.labels = 'Actor'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "CREATE (n:Actor {{id: row.id}})\n",
    "SET n += props;\n",
    "\n",
    "// Import Objects\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "WHERE row.labels = 'Object'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "CREATE (n:Object {{id: row.id}})\n",
    "SET n += props;\n",
    "\n",
    "// Import other node types (add as needed)\n",
    "LOAD CSV WITH HEADERS FROM \"file:///nodes.csv\" AS row\n",
    "WHERE row.labels NOT IN ['Author', 'Book', 'Chapter', 'Chunk', 'Actor', 'Object']\n",
    "WITH row, json.loads(row.properties) as props, row.labels as label\n",
    "CALL {{\n",
    "  WITH row, props, label\n",
    "  WITH \"CREATE (n:\" + label + \" {{id: $id}}) SET n += $props\" as query\n",
    "  CALL query_module.call(query, {{id: row.id, props: props}}) YIELD result\n",
    "  RETURN result\n",
    "}};\n",
    "\n",
    "// Import WRITTEN_BY relationships\n",
    "LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "WHERE row.type = 'WRITTEN_BY'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "MATCH (start {{id: row.start_id}})\n",
    "MATCH (end {{id: row.end_id}})\n",
    "CREATE (start)-[r:WRITTEN_BY]->(end)\n",
    "SET r += props;\n",
    "\n",
    "// Import PART_OF relationships\n",
    "LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "WHERE row.type = 'PART_OF'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "MATCH (start {{id: row.start_id}})\n",
    "MATCH (end {{id: row.end_id}})\n",
    "CREATE (start)-[r:PART_OF]->(end)\n",
    "SET r += props;\n",
    "\n",
    "// Import MENTIONS relationships\n",
    "LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "WHERE row.type = 'MENTIONS'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "MATCH (start {{id: row.start_id}})\n",
    "MATCH (end {{id: row.end_id}})\n",
    "CREATE (start)-[r:MENTIONS]->(end)\n",
    "SET r += props;\n",
    "\n",
    "// Import REFERENCES relationships\n",
    "LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "WHERE row.type = 'REFERENCES'\n",
    "WITH row, json.loads(row.properties) as props\n",
    "MATCH (start {{id: row.start_id}})\n",
    "MATCH (end {{id: row.end_id}})\n",
    "CREATE (start)-[r:REFERENCES]->(end)\n",
    "SET r += props;\n",
    "\n",
    "// Import other relationship types\n",
    "LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "WHERE row.type NOT IN ['WRITTEN_BY', 'PART_OF', 'MENTIONS', 'REFERENCES']\n",
    "WITH row, json.loads(row.properties) as props, row.type as relType\n",
    "MATCH (start {{id: row.start_id}})\n",
    "MATCH (end {{id: row.end_id}})\n",
    "CALL {{\n",
    "  WITH start, end, props, relType\n",
    "  WITH \"CREATE (start)-[r:\" + relType + \"]->(end) SET r += $props\" as query\n",
    "  CALL query_module.call(query, {{props: props}}) YIELD result\n",
    "  RETURN result\n",
    "}};\n",
    "\n",
    "// Verification and sample queries\n",
    "MATCH (n) RETURN labels(n) as label, count(n) as count ORDER BY count DESC;\n",
    "MATCH ()-[r]-() RETURN type(r) as relationship_type, count(r) as count ORDER BY count DESC;\n",
    "MATCH (a:Author) RETURN a.name, a.book LIMIT 5;\n",
    "MATCH (b:Book)-[r:WRITTEN_BY]->(a:Author) RETURN b.name, a.name LIMIT 5;\n",
    "MATCH (c:Chapter)-[r:PART_OF]->(b:Book) RETURN c.name, b.name LIMIT 10;\n",
    "\"\"\"\n",
    "    \n",
    "    script_file = os.path.join(output_dir, \"import_script.cypher\")\n",
    "    simple_script_file = os.path.join(output_dir, \"import_script_manual.cypher\")\n",
    "    \n",
    "    with open(script_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    with open(simple_script_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(simple_script)\n",
    "    \n",
    "    print(f\"✅ Generated Memgraph import script: {script_file}\")\n",
    "    print(f\"✅ Generated manual execution script: {simple_script_file}\")\n",
    "    \n",
    "    return script_file\n",
    "\n",
    "def analyze_memgraph_data(nodes_df: pd.DataFrame, relationships_df: pd.DataFrame):\n",
    "    \"\"\"Analyze the prepared data for Memgraph import.\"\"\"\n",
    "    print(\"\\n📊 Memgraph Import Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if len(nodes_df) > 0:\n",
    "        print(f\"Nodes Summary:\")\n",
    "        print(f\"  Total nodes: {len(nodes_df)}\")\n",
    "        print(f\"  Node labels distribution:\")\n",
    "        label_counts = nodes_df['labels'].value_counts()\n",
    "        for label, count in label_counts.head(10).items():\n",
    "            print(f\"    {label}: {count}\")\n",
    "    \n",
    "    if len(relationships_df) > 0:\n",
    "        print(f\"\\nRelationships Summary:\")\n",
    "        print(f\"  Total relationships: {len(relationships_df)}\")\n",
    "        print(f\"  Relationship types distribution:\")\n",
    "        type_counts = relationships_df['type'].value_counts()\n",
    "        for rel_type, count in type_counts.head(10).items():\n",
    "            print(f\"    {rel_type}: {count}\")\n",
    "\n",
    "def convert_entities_to_memgraph(entities_df: pd.DataFrame, output_dir: str = \"memgraph_import\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Main function to convert entities_df to Memgraph import format.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Converting entities_df to Memgraph import format...\")\n",
    "    \n",
    "    nodes_df = prepare_nodes_for_memgraph(entities_df)\n",
    "    relationships_df = prepare_relationships_for_memgraph(entities_df)\n",
    "    \n",
    "    analyze_memgraph_data(nodes_df, relationships_df)\n",
    "    \n",
    "    nodes_file, relationships_file = export_to_csv(nodes_df, relationships_df, output_dir)\n",
    "    script_file = generate_cypher_import_script(nodes_file, relationships_file, output_dir)\n",
    "    \n",
    "    print(f\"\\n🎉 Conversion complete!\")\n",
    "    print(f\"📁 Files created in '{output_dir}':\")\n",
    "    print(f\"   - nodes.csv: {len(nodes_df)} nodes\")\n",
    "    print(f\"   - relationships.csv: {len(relationships_df)} relationships\") \n",
    "    print(f\"   - import_script.cypher: Cypher import commands\")\n",
    "    print(f\"   - import_script_simple.cypher: Simple version without APOC\")\n",
    "    \n",
    "    return {\n",
    "        'nodes_file': nodes_file,\n",
    "        'relationships_file': relationships_file,\n",
    "        'script_file': script_file,\n",
    "        'output_dir': output_dir\n",
    "    }\n",
    "\n",
    "# MAIN EXECUTION FUNCTION\n",
    "def convert_to_memgraph_complete(entities_df):\n",
    "    \"\"\"\n",
    "    Complete conversion function - call this with your entities_df!\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting complete Memgraph conversion...\")\n",
    "    \n",
    "    # Show input summary\n",
    "    print(f\"\\n📊 Input Summary:\")\n",
    "    print(f\"   Total entities: {len(entities_df)}\")\n",
    "    print(f\"   Nodes: {len(entities_df[entities_df['entity_type'] == 'node'])}\")\n",
    "    print(f\"   Relationships: {len(entities_df[entities_df['entity_type'] == 'relationship'])}\")\n",
    "    print(f\"   Books: {entities_df['book'].nunique()}\")\n",
    "    print(f\"   Authors: {entities_df['author'].nunique()}\")\n",
    "    \n",
    "    # Convert to Memgraph format\n",
    "    memgraph_files = convert_entities_to_memgraph(entities_df)\n",
    "    \n",
    "    # Show instructions\n",
    "    print(f\"\"\"\n",
    "🎯 MEMGRAPH IMPORT INSTRUCTIONS:\n",
    "\n",
    "1. Copy files to Memgraph import directory:\n",
    "   - Usually: /var/lib/memgraph/import/ (Linux)\n",
    "   - Or configure import path in Memgraph\n",
    "\n",
    "2. Open Memgraph Lab (usually http://localhost:3000)\n",
    "\n",
    "3. Upload or copy the import_script.cypher content\n",
    "\n",
    "4. Run the script to import your knowledge graph\n",
    "\n",
    "5. Verify the import:\n",
    "   MATCH (n) RETURN count(n);\n",
    "   MATCH ()-[r]-() RETURN count(r);\n",
    "\n",
    "6. Explore your data:\n",
    "   MATCH (a:Author) RETURN a.name, a.book LIMIT 10;\n",
    "   MATCH (b:Book)-[r:WRITTEN_BY]->(a:Author) RETURN b.name, a.name;\n",
    "   MATCH (c:Chapter)-[r:PART_OF]->(b:Book) RETURN c.name, b.name LIMIT 10;\n",
    "\"\"\")\n",
    "    \n",
    "    return memgraph_files\n",
    "\n",
    "# Usage\n",
    "print(\"✅ Complete Memgraph conversion functions loaded!\")\n",
    "print(\"📞 Call: memgraph_files = convert_to_memgraph_complete(entities_df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting complete Memgraph conversion...\n",
      "\n",
      "📊 Input Summary:\n",
      "   Total entities: 34104\n",
      "   Nodes: 16090\n",
      "   Relationships: 18014\n",
      "   Books: 1\n",
      "   Authors: 1\n",
      "🚀 Converting entities_df to Memgraph import format...\n",
      "📊 Preparing nodes for Memgraph import...\n",
      "🔍 Removed 10785 duplicate nodes\n",
      "✅ Prepared 5305 unique nodes\n",
      "🔗 Preparing relationships for Memgraph import...\n",
      "✅ Prepared 18014 relationships\n",
      "\n",
      "📊 Memgraph Import Analysis:\n",
      "==================================================\n",
      "Nodes Summary:\n",
      "  Total nodes: 5305\n",
      "  Node labels distribution:\n",
      "    Chunk: 2105\n",
      "    Object: 909\n",
      "    Actor: 818\n",
      "    Intangible: 648\n",
      "    Event: 448\n",
      "    Location: 328\n",
      "    Chapter: 45\n",
      "    Book: 2\n",
      "    Author: 2\n",
      "\n",
      "Relationships Summary:\n",
      "  Total relationships: 18014\n",
      "  Relationship types distribution:\n",
      "    CONTAINS: 2622\n",
      "    MENTIONS: 2407\n",
      "    PART_OF: 2064\n",
      "    CONTAINS_DIALOGUE_BY: 1486\n",
      "    WRITTEN_BY: 1220\n",
      "    WROTE: 1204\n",
      "    REFERENCES: 994\n",
      "    INTERACTED_WITH: 660\n",
      "    USED: 602\n",
      "    CONVEYS: 542\n",
      "💾 Exporting to CSV files in 'memgraph_import' directory...\n",
      "✅ Exported 5305 nodes to memgraph_import/nodes.csv\n",
      "✅ Exported 18014 relationships to memgraph_import/relationships.csv\n",
      "📝 Generating Memgraph Cypher import script...\n",
      "✅ Generated Memgraph import script: memgraph_import/import_script.cypher\n",
      "✅ Generated manual execution script: memgraph_import/import_script_manual.cypher\n",
      "\n",
      "🎉 Conversion complete!\n",
      "📁 Files created in 'memgraph_import':\n",
      "   - nodes.csv: 5305 nodes\n",
      "   - relationships.csv: 18014 relationships\n",
      "   - import_script.cypher: Cypher import commands\n",
      "   - import_script_simple.cypher: Simple version without APOC\n",
      "\n",
      "🎯 MEMGRAPH IMPORT INSTRUCTIONS:\n",
      "\n",
      "1. Copy files to Memgraph import directory:\n",
      "   - Usually: /var/lib/memgraph/import/ (Linux)\n",
      "   - Or configure import path in Memgraph\n",
      "\n",
      "2. Open Memgraph Lab (usually http://localhost:3000)\n",
      "\n",
      "3. Upload or copy the import_script.cypher content\n",
      "\n",
      "4. Run the script to import your knowledge graph\n",
      "\n",
      "5. Verify the import:\n",
      "   MATCH (n) RETURN count(n);\n",
      "   MATCH ()-[r]-() RETURN count(r);\n",
      "\n",
      "6. Explore your data:\n",
      "   MATCH (a:Author) RETURN a.name, a.book LIMIT 10;\n",
      "   MATCH (b:Book)-[r:WRITTEN_BY]->(a:Author) RETURN b.name, a.name;\n",
      "   MATCH (c:Chapter)-[r:PART_OF]->(b:Book) RETURN c.name, b.name LIMIT 10;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After running the complete script above:\n",
    "memgraph_files = convert_to_memgraph_complete(entities_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memgraph direct import functions loaded!\n",
      "\n",
      "📞 Usage:\n",
      "# Basic import:\n",
      "success = import_entities_to_memgraph(entities_df)\n",
      "\n",
      "# Clear database and import:\n",
      "success = import_entities_to_memgraph(entities_df, clear_db=True)\n",
      "\n",
      "# Custom connection:\n",
      "success = import_entities_to_memgraph(entities_df, uri='bolt://localhost:7687')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "from typing import Dict, List, Any, Optional\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MemgraphImporter:\n",
    "    \"\"\"Direct importer for Memgraph from entities_df.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, uri: str = \"bolt://localhost:7687\", user: str = \"\", password: str = \"\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize connection to Memgraph.\n",
    "\n",
    "        Args:\n",
    "            uri: Memgraph connection URI (default for Docker)\n",
    "            user: Username (empty for default Memgraph)\n",
    "            password: Password (empty for default Memgraph)\n",
    "        \"\"\"\n",
    "        self.uri = uri\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.driver = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Memgraph.\"\"\"\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(\n",
    "                self.uri, auth=(self.user, self.password)\n",
    "            )\n",
    "\n",
    "            # Test connection\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                test_value = result.single()[\"test\"]\n",
    "                if test_value == 1:\n",
    "                    print(\"✅ Successfully connected to Memgraph!\")\n",
    "                    return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to connect to Memgraph: {e}\")\n",
    "            print(\"💡 Make sure Memgraph Docker is running:\")\n",
    "            print(\"   docker run -p 7687:7687 -p 7444:7444 memgraph/memgraph\")\n",
    "            return False\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close connection to Memgraph.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            print(\"🔌 Disconnected from Memgraph\")\n",
    "\n",
    "    def clear_database(self, confirm: bool = False):\n",
    "        \"\"\"Clear all data from Memgraph.\"\"\"\n",
    "        if not confirm:\n",
    "            print(\"⚠️  To clear database, call with confirm=True\")\n",
    "            return\n",
    "\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"MATCH (n) DETACH DELETE n RETURN count(n) as deleted\")\n",
    "            deleted = result.single()[\"deleted\"]\n",
    "            print(f\"🗑️  Cleared {deleted} nodes from database\")\n",
    "\n",
    "    def create_indexes(self):\n",
    "        \"\"\"Create indexes for better performance.\"\"\"\n",
    "        print(\"📊 Creating indexes...\")\n",
    "\n",
    "        indexes = [\n",
    "            \"CREATE INDEX ON :Author(id);\",\n",
    "            \"CREATE INDEX ON :Book(id);\",\n",
    "            \"CREATE INDEX ON :Chapter(id);\",\n",
    "            \"CREATE INDEX ON :Chunk(id);\",\n",
    "            \"CREATE INDEX ON :Actor(id);\",\n",
    "            \"CREATE INDEX ON :Object(id);\",\n",
    "        ]\n",
    "\n",
    "        with self.driver.session() as session:\n",
    "            for index_query in indexes:\n",
    "                try:\n",
    "                    session.run(index_query)\n",
    "                except Exception as e:\n",
    "                    if \"already exists\" not in str(e).lower():\n",
    "                        print(f\"⚠️  Index creation warning: {e}\")\n",
    "\n",
    "        print(\"✅ Indexes created\")\n",
    "\n",
    "    def clean_properties(\n",
    "        self, properties_json: str, additional_props: Dict[str, Any] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Clean and prepare properties for Memgraph.\"\"\"\n",
    "        try:\n",
    "            if properties_json and properties_json != \"{}\":\n",
    "                props = json.loads(properties_json)\n",
    "            else:\n",
    "                props = {}\n",
    "\n",
    "            if additional_props:\n",
    "                props.update(additional_props)\n",
    "\n",
    "            # Clean properties for Cypher\n",
    "            cleaned_props = {}\n",
    "            for key, value in props.items():\n",
    "                if value is not None and value != \"\":\n",
    "                    # Clean key name for Cypher\n",
    "                    clean_key = (\n",
    "                        key.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "                    )\n",
    "                    clean_key = \"\".join(c for c in clean_key if c.isalnum() or c == \"_\")\n",
    "\n",
    "                    # Format value\n",
    "                    if isinstance(value, (int, float, bool)):\n",
    "                        cleaned_props[clean_key] = value\n",
    "                    else:\n",
    "                        # String - limit length and escape\n",
    "                        str_value = str(value)\n",
    "                        if len(str_value) > 500:\n",
    "                            str_value = str_value[:497] + \"...\"\n",
    "                        cleaned_props[clean_key] = str_value\n",
    "\n",
    "            return cleaned_props\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error cleaning properties: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def import_nodes_batch(self, nodes_data: List[Dict], batch_size: int = 1000):\n",
    "        \"\"\"Import nodes in batches.\"\"\"\n",
    "        print(f\"📦 Importing {len(nodes_data)} nodes in batches of {batch_size}...\")\n",
    "\n",
    "        total_imported = 0\n",
    "\n",
    "        with self.driver.session() as session:\n",
    "            for i in range(0, len(nodes_data), batch_size):\n",
    "                batch = nodes_data[i : i + batch_size]\n",
    "\n",
    "                # Group by label for efficiency\n",
    "                by_label = {}\n",
    "                for node in batch:\n",
    "                    label = node[\"label\"]\n",
    "                    if label not in by_label:\n",
    "                        by_label[label] = []\n",
    "                    by_label[label].append(node)\n",
    "\n",
    "                # Import each label group\n",
    "                for label, label_nodes in by_label.items():\n",
    "                    query = f\"\"\"\n",
    "                    UNWIND $nodes as node\n",
    "                    CREATE (n:{label} {{id: node.id}})\n",
    "                    SET n += node.props\n",
    "                    \"\"\"\n",
    "\n",
    "                    params = {\n",
    "                        \"nodes\": [\n",
    "                            {\"id\": node[\"id\"], \"props\": node[\"props\"]}\n",
    "                            for node in label_nodes\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    try:\n",
    "                        result = session.run(query, params)\n",
    "                        summary = result.consume()\n",
    "                        total_imported += summary.counters.nodes_created\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error importing {label} nodes: {e}\")\n",
    "\n",
    "                # Progress update\n",
    "                print(\n",
    "                    f\"   Processed {min(i + batch_size, len(nodes_data))}/{len(nodes_data)} nodes...\"\n",
    "                )\n",
    "\n",
    "        print(f\"✅ Imported {total_imported} nodes\")\n",
    "        return total_imported\n",
    "\n",
    "    def import_relationships_batch(\n",
    "        self, relationships_data: List[Dict], batch_size: int = 1000\n",
    "    ):\n",
    "        \"\"\"Import relationships in batches.\"\"\"\n",
    "        print(\n",
    "            f\"🔗 Importing {len(relationships_data)} relationships in batches of {batch_size}...\"\n",
    "        )\n",
    "\n",
    "        total_imported = 0\n",
    "\n",
    "        with self.driver.session() as session:\n",
    "            for i in range(0, len(relationships_data), batch_size):\n",
    "                batch = relationships_data[i : i + batch_size]\n",
    "\n",
    "                # Group by relationship type\n",
    "                by_type = {}\n",
    "                for rel in batch:\n",
    "                    rel_type = rel[\"type\"]\n",
    "                    if rel_type not in by_type:\n",
    "                        by_type[rel_type] = []\n",
    "                    by_type[rel_type].append(rel)\n",
    "\n",
    "                # Import each type group\n",
    "                for rel_type, type_rels in by_type.items():\n",
    "                    query = f\"\"\"\n",
    "                    UNWIND $rels as rel\n",
    "                    MATCH (start {{id: rel.start_id}})\n",
    "                    MATCH (end {{id: rel.end_id}})\n",
    "                    CREATE (start)-[r:{rel_type}]->(end)\n",
    "                    SET r += rel.props\n",
    "                    \"\"\"\n",
    "\n",
    "                    params = {\n",
    "                        \"rels\": [\n",
    "                            {\n",
    "                                \"start_id\": rel[\"start_id\"],\n",
    "                                \"end_id\": rel[\"end_id\"],\n",
    "                                \"props\": rel[\"props\"],\n",
    "                            }\n",
    "                            for rel in type_rels\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    try:\n",
    "                        result = session.run(query, params)\n",
    "                        summary = result.consume()\n",
    "                        total_imported += summary.counters.relationships_created\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error importing {rel_type} relationships: {e}\")\n",
    "\n",
    "                print(\n",
    "                    f\"   Processed {min(i + batch_size, len(relationships_data))}/{len(relationships_data)} relationships...\"\n",
    "                )\n",
    "\n",
    "        print(f\"✅ Imported {total_imported} relationships\")\n",
    "        return total_imported\n",
    "\n",
    "    def prepare_nodes_data(self, entities_df: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Prepare node data from entities_df.\"\"\"\n",
    "        print(\"📊 Preparing nodes data...\")\n",
    "\n",
    "        nodes_df = entities_df[entities_df[\"entity_type\"] == \"node\"].copy()\n",
    "        nodes_data = []\n",
    "\n",
    "        for _, row in nodes_df.iterrows():\n",
    "            additional_props = {\n",
    "                \"name\": str(row[\"node_name\"]) if pd.notna(row[\"node_name\"]) else \"\",\n",
    "                \"description\": str(row[\"node_description\"])\n",
    "                if pd.notna(row[\"node_description\"])\n",
    "                else \"\",\n",
    "                \"chapter\": str(row[\"chapter\"]),\n",
    "                \"chunk\": str(row[\"chunk\"]),\n",
    "                \"chunk_order_number\": int(row[\"chunk_order_number\"]),\n",
    "                \"author\": str(row[\"author\"]),\n",
    "                \"book\": str(row[\"book\"]),\n",
    "                \"chunk_index\": int(row.get(\"chunk_index\", 0)),\n",
    "                \"timestamp\": str(row[\"node_timestamp\"])\n",
    "                if pd.notna(row[\"node_timestamp\"])\n",
    "                else \"\",\n",
    "            }\n",
    "\n",
    "            cleaned_props = self.clean_properties(\n",
    "                row[\"node_properties\"], additional_props\n",
    "            )\n",
    "\n",
    "            nodes_data.append(\n",
    "                {\n",
    "                    \"id\": row[\"node_id\"],\n",
    "                    \"label\": row[\"node_label\"],\n",
    "                    \"props\": cleaned_props,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Remove duplicates by ID\n",
    "        seen_ids = set()\n",
    "        unique_nodes = []\n",
    "        for node in nodes_data:\n",
    "            if node[\"id\"] not in seen_ids:\n",
    "                seen_ids.add(node[\"id\"])\n",
    "                unique_nodes.append(node)\n",
    "\n",
    "        print(\n",
    "            f\"✅ Prepared {len(unique_nodes)} unique nodes (removed {len(nodes_data) - len(unique_nodes)} duplicates)\"\n",
    "        )\n",
    "        return unique_nodes\n",
    "\n",
    "    def prepare_relationships_data(self, entities_df: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Prepare relationship data from entities_df.\"\"\"\n",
    "        print(\"🔗 Preparing relationships data...\")\n",
    "\n",
    "        rels_df = entities_df[entities_df[\"entity_type\"] == \"relationship\"].copy()\n",
    "        relationships_data = []\n",
    "\n",
    "        for _, row in rels_df.iterrows():\n",
    "            additional_props = {\n",
    "                \"weight\": float(row[\"relationship_weight\"])\n",
    "                if pd.notna(row[\"relationship_weight\"])\n",
    "                else 1.0,\n",
    "                \"chapter\": str(row[\"chapter\"]),\n",
    "                \"chunk\": str(row[\"chunk\"]),\n",
    "                \"chunk_order_number\": int(row[\"chunk_order_number\"]),\n",
    "                \"author\": str(row[\"author\"]),\n",
    "                \"book\": str(row[\"book\"]),\n",
    "                \"chunk_index\": int(row.get(\"chunk_index\", 0)),\n",
    "                \"timestamp\": str(row[\"relationship_timestamp\"])\n",
    "                if pd.notna(row[\"relationship_timestamp\"])\n",
    "                else \"\",\n",
    "            }\n",
    "\n",
    "            cleaned_props = self.clean_properties(\n",
    "                row[\"relationship_properties\"], additional_props\n",
    "            )\n",
    "\n",
    "            relationships_data.append(\n",
    "                {\n",
    "                    \"start_id\": row[\"start_node_id\"],\n",
    "                    \"end_id\": row[\"end_node_id\"],\n",
    "                    \"type\": row[\"relationship_type\"],\n",
    "                    \"props\": cleaned_props,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Filter out relationships with missing nodes\n",
    "        valid_rels = [\n",
    "            r\n",
    "            for r in relationships_data\n",
    "            if r[\"start_id\"]\n",
    "            and r[\"end_id\"]\n",
    "            and str(r[\"start_id\"]) != \"nan\"\n",
    "            and str(r[\"end_id\"]) != \"nan\"\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            f\"✅ Prepared {len(valid_rels)} relationships (filtered {len(relationships_data) - len(valid_rels)} invalid)\"\n",
    "        )\n",
    "        return valid_rels\n",
    "\n",
    "    def verify_import(self):\n",
    "        \"\"\"Verify the imported data.\"\"\"\n",
    "        print(\"\\n🔍 Verifying import...\")\n",
    "\n",
    "        with self.driver.session() as session:\n",
    "            # Count nodes\n",
    "            result = session.run(\n",
    "                \"MATCH (n) RETURN labels(n) as label, count(n) as count ORDER BY count DESC\"\n",
    "            )\n",
    "            print(\"📊 Node counts by label:\")\n",
    "            for record in result:\n",
    "                label = record[\"label\"][0] if record[\"label\"] else \"Unknown\"\n",
    "                count = record[\"count\"]\n",
    "                print(f\"   {label}: {count}\")\n",
    "\n",
    "            # Count relationships\n",
    "            result = session.run(\n",
    "                \"MATCH ()-[r]-() RETURN type(r) as rel_type, count(r) as count ORDER BY count DESC\"\n",
    "            )\n",
    "            print(\"\\n🔗 Relationship counts by type:\")\n",
    "            for record in result:\n",
    "                rel_type = record[\"rel_type\"]\n",
    "                count = record[\"count\"]\n",
    "                print(f\"   {rel_type}: {count}\")\n",
    "\n",
    "            # Sample queries\n",
    "            print(\"\\n📖 Sample data:\")\n",
    "\n",
    "            # Authors\n",
    "            result = session.run(\"MATCH (a:Author) RETURN a.name, a.book LIMIT 3\")\n",
    "            for record in result:\n",
    "                print(f\"   Author: {record['a.name']} - Book: {record['a.book']}\")\n",
    "\n",
    "            # Books and authors\n",
    "            result = session.run(\n",
    "                \"MATCH (b:Book)-[r:WRITTEN_BY]->(a:Author) RETURN b.name, a.name LIMIT 3\"\n",
    "            )\n",
    "            for record in result:\n",
    "                print(f\"   Book: {record['b.name']} by {record['a.name']}\")\n",
    "\n",
    "\n",
    "def import_entities_to_memgraph(\n",
    "    entities_df: pd.DataFrame,\n",
    "    uri: str = \"bolt://localhost:7687\",\n",
    "    clear_db: bool = False,\n",
    "    batch_size: int = 1000,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Main function to import entities_df directly to Memgraph.\n",
    "\n",
    "    Args:\n",
    "        entities_df: DataFrame with nodes and relationships\n",
    "        uri: Memgraph connection URI\n",
    "        clear_db: Whether to clear existing data\n",
    "        batch_size: Batch size for imports\n",
    "\n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting direct import to Memgraph...\")\n",
    "\n",
    "    importer = MemgraphImporter(uri)\n",
    "\n",
    "    try:\n",
    "        # Connect\n",
    "        if not importer.connect():\n",
    "            return False\n",
    "\n",
    "        # Clear database if requested\n",
    "        if clear_db:\n",
    "            importer.clear_database(confirm=True)\n",
    "\n",
    "        # Create indexes\n",
    "        importer.create_indexes()\n",
    "\n",
    "        # Prepare data\n",
    "        nodes_data = importer.prepare_nodes_data(entities_df)\n",
    "        relationships_data = importer.prepare_relationships_data(entities_df)\n",
    "\n",
    "        # Import data\n",
    "        start_time = time.time()\n",
    "\n",
    "        nodes_imported = importer.import_nodes_batch(nodes_data, batch_size)\n",
    "        relationships_imported = importer.import_relationships_batch(\n",
    "            relationships_data, batch_size\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Verify\n",
    "        importer.verify_import()\n",
    "\n",
    "        print(f\"\\n🎉 Import completed successfully!\")\n",
    "        print(f\"   ⏱️  Total time: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"   📊 Nodes imported: {nodes_imported}\")\n",
    "        print(f\"   🔗 Relationships imported: {relationships_imported}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Import failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    finally:\n",
    "        importer.close()\n",
    "\n",
    "\n",
    "# Usage functions\n",
    "print(\"✅ Memgraph direct import functions loaded!\")\n",
    "print(\"\\n📞 Usage:\")\n",
    "print(\"# Basic import:\")\n",
    "print(\"success = import_entities_to_memgraph(entities_df)\")\n",
    "print(\"\\n# Clear database and import:\")\n",
    "print(\"success = import_entities_to_memgraph(entities_df, clear_db=True)\")\n",
    "print(\"\\n# Custom connection:\")\n",
    "print(\"success = import_entities_to_memgraph(entities_df, uri='bolt://localhost:7687')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting direct import to Memgraph...\n",
      "✅ Successfully connected to Memgraph!\n",
      "🗑️  Cleared 9 nodes from database\n",
      "📊 Creating indexes...\n",
      "✅ Indexes created\n",
      "📊 Preparing nodes data...\n",
      "✅ Prepared 5305 unique nodes (removed 10785 duplicates)\n",
      "🔗 Preparing relationships data...\n",
      "✅ Prepared 18014 relationships (filtered 0 invalid)\n",
      "📦 Importing 5305 nodes in batches of 1000...\n",
      "   Processed 1000/5305 nodes...\n",
      "   Processed 2000/5305 nodes...\n",
      "   Processed 3000/5305 nodes...\n",
      "   Processed 4000/5305 nodes...\n",
      "   Processed 5000/5305 nodes...\n",
      "   Processed 5305/5305 nodes...\n",
      "✅ Imported 5305 nodes\n",
      "🔗 Importing 18014 relationships in batches of 1000...\n",
      "   Processed 1000/18014 relationships...\n",
      "   Processed 2000/18014 relationships...\n",
      "   Processed 3000/18014 relationships...\n",
      "   Processed 4000/18014 relationships...\n",
      "   Processed 5000/18014 relationships...\n",
      "   Processed 6000/18014 relationships...\n",
      "   Processed 7000/18014 relationships...\n",
      "   Processed 8000/18014 relationships...\n",
      "   Processed 9000/18014 relationships...\n",
      "   Processed 10000/18014 relationships...\n",
      "   Processed 11000/18014 relationships...\n",
      "   Processed 12000/18014 relationships...\n",
      "   Processed 13000/18014 relationships...\n",
      "   Processed 14000/18014 relationships...\n",
      "   Processed 15000/18014 relationships...\n",
      "   Processed 16000/18014 relationships...\n",
      "   Processed 17000/18014 relationships...\n",
      "   Processed 18000/18014 relationships...\n",
      "   Processed 18014/18014 relationships...\n",
      "✅ Imported 18014 relationships\n",
      "\n",
      "🔍 Verifying import...\n",
      "📊 Node counts by label:\n",
      "   Chunk: 2105\n",
      "   Object: 909\n",
      "   Actor: 818\n",
      "   Intangible: 648\n",
      "   Event: 448\n",
      "   Location: 328\n",
      "   Chapter: 45\n",
      "   Author: 2\n",
      "   Book: 2\n",
      "\n",
      "🔗 Relationship counts by type:\n",
      "   CONTAINS: 5243\n",
      "   MENTIONS: 4814\n",
      "   PART_OF: 4127\n",
      "   CONTAINS_DIALOGUE_BY: 2954\n",
      "   WRITTEN_BY: 2440\n",
      "   WROTE: 2408\n",
      "   REFERENCES: 1988\n",
      "   INTERACTED_WITH: 1281\n",
      "   USED: 1204\n",
      "   CONVEYS: 1084\n",
      "   DESCRIBES: 792\n",
      "   ASSOCIATED_WITH: 736\n",
      "   LOCATED_AT: 622\n",
      "   MENTIONED: 560\n",
      "   DEPICTS: 476\n",
      "   PARTICIPATED_IN: 466\n",
      "   FEATURES: 322\n",
      "   CAUSED_BY: 300\n",
      "   AFFECTED: 290\n",
      "   INFLUENCED: 270\n",
      "   INVOLVED_IN: 222\n",
      "   LEARNED: 212\n",
      "   BELONGS_TO: 172\n",
      "   LOCATED_IN: 166\n",
      "   OCCURRED_AT: 156\n",
      "   INFLUENCED_BY: 134\n",
      "   KNOWN_BY: 128\n",
      "   ARRIVED_AT: 128\n",
      "   DEPARTED_FROM: 128\n",
      "   INVOLVED: 128\n",
      "   ACQUIRED: 116\n",
      "   CREATED: 116\n",
      "   CLAIMED: 96\n",
      "   CAUSED: 92\n",
      "   INFLUENCES: 90\n",
      "   REFUTED: 84\n",
      "   POSSESSED_BY: 84\n",
      "   HOSTS: 72\n",
      "   REVEALED: 58\n",
      "   REPRESENTS: 56\n",
      "   RESULTED_IN: 54\n",
      "   EXPERIENCED: 52\n",
      "   RELATED_TO: 49\n",
      "   HOSTED: 48\n",
      "   SET_IN: 46\n",
      "   MENTIONED_IN: 46\n",
      "   PRECEDED_BY: 39\n",
      "   REVEALED_BY: 32\n",
      "   APPLIED_TO: 30\n",
      "   ADJACENT_TO: 30\n",
      "   PRECEDED: 26\n",
      "   MOVED_TO: 26\n",
      "   DE_ACQUIRED: 26\n",
      "   MANIFESTED_IN: 24\n",
      "   FELT: 24\n",
      "   FOLLOWED_BY: 23\n",
      "   CONNECTED_TO: 21\n",
      "   POSSESSED: 20\n",
      "   HAD: 20\n",
      "   FEARED: 20\n",
      "   REFERS_TO: 18\n",
      "   SEEKS: 16\n",
      "   REQUESTED: 16\n",
      "   ORIGINATED_FROM: 14\n",
      "   EXPLORES: 14\n",
      "   PRODUCED: 14\n",
      "   CONTRADICTS: 14\n",
      "   DERIVED_FROM: 14\n",
      "   DEMONSTRATED_IN: 12\n",
      "   HAS: 12\n",
      "   WITNESSED: 12\n",
      "   WANTS: 8\n",
      "   EXPRESSED: 8\n",
      "   DESIRED: 8\n",
      "   GAVE: 8\n",
      "   MODIFIED: 6\n",
      "   ATTRACTS: 6\n",
      "   FOLLOWED: 6\n",
      "   SUPPORTS: 6\n",
      "   AFFECTED_BY: 6\n",
      "   EXPERIENCES: 6\n",
      "   MOVED_FROM: 6\n",
      "   PROVIDES: 6\n",
      "   CONSUMED: 6\n",
      "   TRIED: 6\n",
      "   FEELS: 6\n",
      "   CONTEMPORARY_OF: 4\n",
      "   SENT: 4\n",
      "   IS: 4\n",
      "   FORGOT: 4\n",
      "   INTO: 4\n",
      "   DID_NOT: 4\n",
      "   EMBODIES: 4\n",
      "   HEARD: 4\n",
      "   REALIZED: 4\n",
      "   POSSESSES: 4\n",
      "   DEMONSTRATED: 4\n",
      "   USED_BY: 4\n",
      "   WANTED: 4\n",
      "   OWNED: 4\n",
      "   CLIMBED: 2\n",
      "   TOWARDS: 2\n",
      "   AVOID: 2\n",
      "   WISHES: 2\n",
      "   WISHED: 2\n",
      "   NEEDED: 2\n",
      "   OBSERVED: 2\n",
      "   AVOIDED: 2\n",
      "   LOST: 2\n",
      "   ASKS_ABOUT: 2\n",
      "   DESERVES: 2\n",
      "   EXPERIENCING: 2\n",
      "   TOLD: 2\n",
      "   ANTICIPATES: 2\n",
      "   SEEKING: 2\n",
      "   MAKES: 2\n",
      "   GOING_TO_GET: 2\n",
      "   PRACTICED: 2\n",
      "   BECOMES: 2\n",
      "   INTERPRETED: 2\n",
      "   TARGET: 2\n",
      "   FUMBLED_WITH: 2\n",
      "   ANTICIPATED: 2\n",
      "   BELIEVED: 2\n",
      "   IN_CHAPTER: 2\n",
      "   LIVED_IN: 2\n",
      "   FOUND: 2\n",
      "   SOLD: 2\n",
      "   REFLECTS_ON: 2\n",
      "   TRY_TO_DO: 2\n",
      "   APPLIES_TO: 2\n",
      "   SIMILAR_TO: 2\n",
      "   WORE: 2\n",
      "   RAISED_CAIN: 2\n",
      "   KNOWS: 2\n",
      "   WANTED_TO_GO: 2\n",
      "   ASKED: 2\n",
      "   RISKING: 2\n",
      "   INVESTED: 2\n",
      "   WENT_TO: 2\n",
      "   SPOKE_TO: 2\n",
      "   OFFERS: 2\n",
      "   TARGETS: 2\n",
      "   HOUSES: 2\n",
      "   SEARCHED_FOR: 2\n",
      "   DEMONSTRATES: 2\n",
      "   INQUIRED_ABOUT: 2\n",
      "   OCCURS_AT: 2\n",
      "   CONCURRENT_WITH: 2\n",
      "   DESTINATION_OF: 2\n",
      "   RESULTED_FROM: 2\n",
      "   OFFERED: 2\n",
      "   REVEALS: 2\n",
      "   THOUGHT_ABOUT: 2\n",
      "   REFUSED: 2\n",
      "   READ: 2\n",
      "   TEACHES: 2\n",
      "   SEEKED: 2\n",
      "   DESIRES: 2\n",
      "   GET_OUT_OF: 2\n",
      "   MENTIONED_BY: 2\n",
      "   REFERENCED_BY: 2\n",
      "   INSPIRED_BY: 2\n",
      "   REFERENCE: 2\n",
      "   RETRIEVES: 2\n",
      "   TRUSTED: 2\n",
      "   ENGAGED_IN: 2\n",
      "   FEARS: 2\n",
      "   RECEIVED: 2\n",
      "   DEPRIVED_OF: 2\n",
      "   RECEIVES: 2\n",
      "   GET_AWAY_FROM: 2\n",
      "   ASKED_ABOUT: 2\n",
      "   DESTINATION: 2\n",
      "   CONTAINS_DIALOGUE: 2\n",
      "\n",
      "📖 Sample data:\n",
      "   Author: Mark Twain - Book: ADVENTURES OF HUCKLEBERRY FINN\n",
      "   Author: Shakespeare - Book: ADVENTURES OF HUCKLEBERRY FINN\n",
      "   Book: ADVENTURES OF HUCKLEBERRY FINN by Mark Twain\n",
      "   Book: ADVENTURES OF HUCKLEBERRY FINN by Mark Twain\n",
      "   Book: ADVENTURES OF HUCKLEBERRY FINN by Mark Twain\n",
      "\n",
      "🎉 Import completed successfully!\n",
      "   ⏱️  Total time: 33.85 seconds\n",
      "   📊 Nodes imported: 5305\n",
      "   🔗 Relationships imported: 18014\n",
      "🔌 Disconnected from Memgraph\n"
     ]
    }
   ],
   "source": [
    "success = import_entities_to_memgraph(entities_df, clear_db=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
